<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Roger&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Roger&#39;s Blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Roger&#39;s Blog">





  
  
  <link rel="canonical" href="http://yoursite.com/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>Roger's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Roger's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-commonweal">

    
    
      
    

    

    <a href="/404" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>公益 404</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/Java大数据开发入门系列-四-————Spark之RDD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/Java大数据开发入门系列-四-————Spark之RDD/" class="post-title-link" itemprop="url">Java大数据开发入门系列(四)————Spark之RDD</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-09-14 21:48:33" itemprop="dateCreated datePublished" datetime="2020-09-14T21:48:33+08:00">2020-09-14</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-09-15 22:17:36" itemprop="dateModified" datetime="2020-09-15T22:17:36+08:00">2020-09-15</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="RDD简介"><a href="#RDD简介" class="headerlink" title="RDD简介"></a>RDD简介</h2><p><code>RDD</code> 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象。可以简单地把 RDD 理解成一个提供了许多操作接口的数据集合。和一般数据集不同的是，其实际数据被划分为一到多个分区,所有介区数据分布存储于一批机器中(内存或磁盘中),这里的分区可以简单地和 Hadoop HDFS 里面的文件块来对比理解。如图所示：</p>
<p><img src="http://img.rogermaster.top/uPic/1600093597352.jpg" alt="1600093597352"></p>
<p>定义了一个名字为“myRDD”的 RDD 数据集，这个数据集被切分成了多个分区（Partion，可以对比 HDFS 的 Block 的概念来理解），可能每个分区实际存储在不同的机器上，同时也可能存储在内存（Memory）或硬盘上（HDFS，当然也可能是其他分布式文件系统）。</p>
<p>RDD是只读的、分区记录的集合，支持并行操作，可以由外部数据集或其他 RDD 转换而来，它具有以下特性：</p>
<ul>
<li>一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数；</li>
<li>RDD 拥有一个用于计算分区的函数 compute；</li>
<li>RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算；</li>
<li>Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)；</li>
<li>一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<h2 id="RDD两大类操作（spark-常用的算子）"><a href="#RDD两大类操作（spark-常用的算子）" class="headerlink" title="RDD两大类操作（spark 常用的算子）"></a>RDD两大类操作（spark 常用的算子）</h2><p>RDD主要有两大类操作，分别为转换操作（Tiansformations）和行动操作（Actions)）。转换操作主要是指把原始数据集加载到 RDD 以及把一个 RDD 转换为另外一个 RDD，而行动操作主要指把RDD 存储到硬盘或触发转换执行。例如，map 是一个 Transformation 操作，该操作作用于数据集上的每一个元素，并且返回一个新的 RDD 作为结果。而 reduce 是一个 Action 操作,该操作通过一些函数聚合 RDD 中的所有元素并且返回最终的结果给 Driver。</p>
<h3 id="spark-常用的-Transformation-算子"><a href="#spark-常用的-Transformation-算子" class="headerlink" title="spark 常用的 Transformation 算子"></a>spark 常用的 Transformation 算子</h3><table>
<thead>
<tr>
<th>Transformation 算子</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素运用 <em>func</em> 函数，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素使用<em>func</em> 函数进行过滤，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>与 map 类似，但是每一个输入的 item 被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 Seq ）。</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为 Iterator<t> =&gt; Iterator<u> ，其中 T 是 RDD 的类型，即 RDD[T]</u></t></td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td>与 mapPartitions 类似，但 <em>func</em> 类型为 (Int, Iterator<t>) =&gt; Iterator<u> ，其中第一个参数为分区索引</u></t></td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）；</td>
</tr>
<tr>
<td><strong>union</strong>(<em>otherDataset</em>)</td>
<td>合并两个 RDD</td>
</tr>
<tr>
<td><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td>求两个 RDD 的交集</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numTasks</em>]))</td>
<td>去重</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>
<td>按照 key 值进行分区，即在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, Iterable<v>) <strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 性能会更好 <strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</v></td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>
<td>按照 key 值进行分组，并对分组后的数据执行归约操作。</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>
<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数进行配置。</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>
<td>按照 key 进行排序，其中的 key 需要实现 Ordered 特质，即可比较</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>
</tr>
<tr>
<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, (Iterable<v>, Iterable<w>)) tuples 的 dataset。</w></v></td>
</tr>
<tr>
<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>
<td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) 类型的 dataset（即笛卡尔积）。</td>
</tr>
<tr>
<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td>将 RDD 中的分区数减少为 numPartitions。</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>随机重新调整 RDD 中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>
<td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并对分区中的数据按照 key 值进行排序。这比调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作所在的机器。</td>
</tr>
</tbody></table>
<h3 id="Spark-常用的-Action-算子"><a href="#Spark-常用的-Action-算子" class="headerlink" title="Spark 常用的 Action 算子"></a>Spark 常用的 Action 算子</h3><table>
<thead>
<tr>
<th>Action（动作）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>使用函数<em>func</em>执行归约操作</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>以一个 array 数组的形式返回 dataset 的所有元素，适用于小结果集。</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回 dataset 中元素的个数。</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回 dataset 中的第一个元素，等价于 take(1)。</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>
<td>对一个 dataset 进行随机抽样</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。该操作要求 RDD 中的元素需要实现 Hadoop 的 Writable 接口。对于 Scala 语言而言，它可以将 Spark 中的基本数据类型自动隐式转换为对应 Writable 类型。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td>使用 Java 序列化后存储，可以使用 <code>SparkContext.objectFile()</code> 进行加载。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>计算每个键出现的次数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>遍历 RDD 中每个元素，并对其执行<em>fun</em>函数b</td>
</tr>
</tbody></table>
<p>所有的转换都是懒惰（Lazy）操作，它们只是记住了需要这样的转换操作，并不会马上执行，只有等到 Actions 操作的时候才会真正启动计算过程进行计算。举个具体的例子，如图所示，</p>
<p><img src="http://img.rogermaster.top/uPic/1600094733358.jpg" alt="1600094733358"></p>
<p>先经过转换 textFile 把数据从 HDFS 加载到 RDDA 以及 RDDC，这时其实RDD A 或者 RDD C 中都是没有数据的。再到后面的转换 flatMap、map、reduceByKey等，分别把 RDD A 转换为 RDD B 到 RDD F 以及 RDD C 到 RDD E等,这些转换都是没有执行的。可以理解为先做个计划，但是没有具体执行，等到执行操作saveAsSequenceFile时，才开始真正触发并执行任务。</p>
<h2 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h2><p>RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：</p>
<ul>
<li><strong>窄依赖 (narrow dependency)</strong>：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；</li>
<li><strong>宽依赖 (wide dependency)</strong>：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。</li>
</ul>
<p>如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：</p>
<p><img src="http://img.rogermaster.top/uPic/EMhehb.jpg" alt="EMhehb"></p>
<p>区分这两种依赖是非常有用的：</p>
<ul>
<li>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。</li>
<li>窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。</li>
</ul>
<h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><p>Spark 中还有一个重要的概念，即 Stage。一般而言,一个 Job 会分成一定数量的 Stage。各个 Stage 之间按照顺序执行。</p>
<p>那么 Siage 是怎么划分的？</p>
<p>在 Spark 中，一个 Job 会被拆分成多组 Task，每组任务就是一个 Stage。而 Spark 中有两类 Task，分别是 ShuffleMapTask和 ResultTask。ShuffleMapTask 的输出是 Shuffle 所需的数据，ResultTask 的输出是最终的结果。</p>
<p>因此 Stage 也以此为依据进行划分，简单地说，Stage 是以 Shuffle 和 Result 这两种类型划分的，Shuffle 之前的所有变换是一个 Stage，Shuftle 之后的操作是另一个 Stage。比如</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.parallize(<span class="number">1</span> to <span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure>

<p>这个操作没有 Shufile,直接就输出了。它的 Task 只有一个，即 ResultTask，Stage 也只有一个。如果是</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(x=&gt;(x, <span class="number">1</span>).reduceByKey(_ + _).foreach(println)</span><br></pre></td></tr></table></figure>

<p>这个 Job 因为有 reduceByKey 操作，所以有一个 Shuffie 过程，那么 reduceBykey之前的是一个 Stage，执行 shuffleMapTask，输出 Shufle 所需的数据reduceByKey 到最后是一个 Stage，直接就输出结果了。如果 Job 中有多个 Shufle，那么每个 Shufle 之前都是一个 Stage。</p>
<h2 id="Job划分"><a href="#Job划分" class="headerlink" title="Job划分"></a>Job划分</h2><p><img src="http://img.rogermaster.top/uPic/aKk0mk.png" alt="aKk0mk"></p>
<p>如图所示，这是一个 Job 的划分过程。在图中，可以看到有 3 个阶段（Stage）分别是 Stage 1（RDD A )、Stage 2( RDD C、RDD D、RDD E、RDD F)、Stage 3（包含所有 RDD）Spark 会将每一个 Job 划分为多个不同的 Stage,而 Stage 之间的依赖关系则形成了有向无环图(DAG )。对于窄依赖，Spark 会尽量多地将 RDD 转换放在同一个阶段（Stage）中。而对于宽依赖,由于宽依赖通常意味着 Shuffle操作,因此Spark会将Shuffle操作定义为阶段( Stage)的边界。也就是说，Spark 遇到宽依赖就划分为一个 Stage，遇到窄依赖则将这个 RDD 加入到该 Stage 中。因此在图中，RDD C、RDD D、RDD E、RDD F 被构建在一个 Stage 中，RDD A 被构建在一个单独的 Stage 中，而 RDDB 和 RDDG 又被构建在同一个 Stage 中。</p>
<h2 id="RDD调度运行流程"><a href="#RDD调度运行流程" class="headerlink" title="RDD调度运行流程"></a>RDD调度运行流程</h2><p>结合前面的介绍，针对 Spark的RDD调度运行流程简单解释如下。</p>
<p><img src="http://img.rogermaster.top/uPic/1600096397045.jpg" alt="1600096397045"></p>
<p>如图所示,用户代码(如rdd1.join…)转换为有向无环图（DAG）后，交给 DAGScheduler，由它把 RDD 的有向无环图分割成各个 Stage 的有向无环图，形成 TaskSet，再提交给 TaskScheduler，由 TaskSeheduler 把任务（Task）提交给每个 Worker 上的 Executor，执行具体的 Task。在 TaskSeheduler 中，是不知道各个 Stage 的存在的，运行的只有 Task。</p>
<h2 id="RDD内存管理（持久化）"><a href="#RDD内存管理（持久化）" class="headerlink" title="RDD内存管理（持久化）"></a>RDD内存管理（持久化）</h2><p>Spark最重要的一个功能是它可以通过各种操作（operations）持久化（或者缓存）一个集合到内存中。当你持久化一个RDD的时候，每一个节点都将参与计算的所有分区数据存储到内存中，并且这些 数据可以被这个集合（以及这个集合衍生的其他集合）的动作（action）重复利用。这个能力使后续的动作速度更快（通常快10倍以上）。对应迭代算法和快速的交互使用来说，缓存是一个关键的工具。</p>
<p>你能通过<code>persist()</code>或者<code>cache()</code>方法持久化一个rdd。首先，在action中计算得到rdd；然后，将其保存在每个节点的内存中。Spark的缓存是一个容错的技术-如果RDD的任何一个分区丢失，它 可以通过原有的转换（transformations）操作自动的重复计算并且创建出这个分区。</p>
<p>此外，我们可以利用不同的存储级别存储每一个被持久化的RDD。例如，它允许我们持久化集合到磁盘上、将集合作为序列化的Java对象持久化到内存中、在节点间复制集合或者存储集合到 <a href="http://tachyon-project.org/" target="_blank" rel="noopener">Tachyon</a>中。我们可以通过传递一个<code>StorageLevel</code>对象给<code>persist()</code>方法设置这些存储级别。<code>cache()</code>方法使用了默认的存储级别—<code>StorageLevel.MEMORY_ONLY</code>。完整的存储级别介绍如下所示：</p>
<table>
<thead>
<tr>
<th>Storage Level</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>将RDD作为非序列化的Java对象存储在jvm中。如果RDD不适合存在内存中，一些分区将不会被缓存，从而在每次需要这些分区时都需重新计算它们。这是系统默认的存储级别。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>将RDD作为非序列化的Java对象存储在jvm中。如果RDD不适合存在内存中，将这些不适合存在内存中的分区存储在磁盘中，每次需要时读出它们。</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>将RDD作为序列化的Java对象存储（每个分区一个byte数组）。这种方式比非序列化方式更节省空间，特别是用到快速的序列化工具时，但是会更耗费cpu资源—密集的读操作。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>和MEMORY_ONLY_SER类似，但不是在每次需要时重复计算这些不适合存储到内存中的分区，而是将这些分区存储到磁盘中。</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>仅仅将RDD分区存储到磁盘中</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td>
<td>和上面的存储级别类似，但是复制每个分区到集群的两个节点上面</td>
</tr>
<tr>
<td>OFF_HEAP (experimental)</td>
<td>以序列化的格式存储RDD到<a href="http://tachyon-project.org/" target="_blank" rel="noopener">Tachyon</a>中。相对于MEMORY_ONLY_SER，OFF_HEAP减少了垃圾回收的花费，允许更小的执行者共享内存池。这使其在拥有大量内存的环境下或者多并发应用程序的环境中具有更强的吸引力。</td>
</tr>
</tbody></table>
<p>NOTE:在python中，存储的对象都是通过Pickle库序列化了的，所以是否选择序列化等级并不重要。</p>
<p>Spark也会自动持久化一些shuffle操作（如<code>reduceByKey</code>）中的中间数据，即使用户没有调用<code>persist</code>方法。这样的好处是避免了在shuffle出错情况下，需要重复计算整个输入。如果用户计划重用 计算过程中产生的RDD，我们仍然推荐用户调用<code>persist</code>方法。</p>
<h4 id="如何选择存储级别"><a href="#如何选择存储级别" class="headerlink" title="如何选择存储级别"></a>如何选择存储级别</h4><p>Spark的多个存储级别意味着在内存利用率和cpu利用效率间的不同权衡。我们推荐通过下面的过程选择一个合适的存储级别：</p>
<ul>
<li><p>如果你的RDD适合默认的存储级别（MEMORY_ONLY），就选择默认的存储级别。因为这是cpu利用率最高的选项，会使RDD上的操作尽可能的快。</p>
</li>
<li><p>如果不适合用默认的级别，选择MEMORY_ONLY_SER。选择一个更快的序列化库提高对象的空间使用率，但是仍能够相当快的访问。</p>
</li>
<li><p>除非函数计算RDD的花费较大或者它们需要过滤大量的数据，不要将RDD存储到磁盘上，否则，重复计算一个分区就会和重磁盘上读取数据一样慢。</p>
</li>
<li><p>如果你希望更快的错误恢复，可以利用重复(replicated)存储级别。所有的存储级别都可以通过重复计算丢失的数据来支持完整的容错，但是重复的数据能够使你在RDD上继续运行任务，而不需要重复计算丢失的数据。</p>
</li>
<li><p>在拥有大量内存的环境中或者多应用程序的环境中，OFF_HEAP具有如下优势：</p>
<ul>
<li><p>它运行多个执行者共享Tachyon中相同的内存池</p>
</li>
<li><p>它显著地减少垃圾回收的花费</p>
</li>
<li><p>如果单个的执行者崩溃，缓存的数据不会丢失</p>
</li>
</ul>
</li>
</ul>
<h4 id="回收策略"><a href="#回收策略" class="headerlink" title="回收策略"></a>回收策略</h4><p>为了管理有限的内存资源，我们在 RDD的层面上采用 LRU（最近最少使用）回收策略。当一个新的 RDD 分区被计算但是没有足够的内存空间来存储这个分区的数据的时候，我们回收掉最近很少使用的 RDD 的分区数据的占用内存，如果这个 RDD 和这个新的计算分区的 RDD 时同一个 RDD 的时候，我们则不对这个分区数据占用的内存做回收。在这种情况下，我们将相同的 RDD 的老分区的数据保存在内存中是为了不让老是重新计算这些分区的数据，这点是非常重要的，因为很多操作都是对整个 RDD 的所有的 tasks 进行计算的，所以非常有必要将后续要用到的数据保存在内存中。到目前为止，我们发现这种默认的机制在所有的应用中工作的很好，但是我们还是将持久每一个 RDD 数据的策略的控制权交给用户。</p>
<p>测试代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.xpleaf.bigdata.spark.scala.core.p3</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Spark RDD的持久化</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">_01SparkPersistOps</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(_01SparkPersistOps.getClass.getSimpleName())</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">        <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> start = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="keyword">val</span> linesRDD = sc.textFile(<span class="string">"D:/data/spark/sequences.txt"</span>)</span><br><span class="line">        <span class="comment">// linesRDD.cache()</span></span><br><span class="line">        <span class="comment">// linesRDD.persist(StorageLevel.MEMORY_ONLY)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行第一次RDD的计算</span></span><br><span class="line">        <span class="keyword">val</span> retRDD = linesRDD.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">// retRDD.cache()</span></span><br><span class="line">        <span class="comment">// retRDD.persist(StorageLevel.DISK_ONLY)</span></span><br><span class="line">        retRDD.count()</span><br><span class="line">        println(<span class="string">"第一次计算消耗的时间："</span> + (<span class="type">System</span>.currentTimeMillis() - start) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行第二次RDD的计算</span></span><br><span class="line">        start = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="comment">// linesRDD.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).count()</span></span><br><span class="line">        retRDD.count()</span><br><span class="line">        println(<span class="string">"第二次计算消耗的时间："</span> + (<span class="type">System</span>.currentTimeMillis() - start) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 持久化使用结束之后，要想卸载数据</span></span><br><span class="line">        <span class="comment">// linesRDD.unpersist()</span></span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="检查点支持"><a href="#检查点支持" class="headerlink" title="检查点支持"></a>检查点支持</h2><p>虽然“血缘关系”可以用于错误后RDD的恢复，但是对于很长的“血缘关系”的RDD来说，这样的恢复耗时比较长，因此需要通过检查点操作(Checkpoint)保存到外部存储中。</p>
<p>通常情况下，对于包含宽依赖的长“血缘关系”的RDD设置检查点操作是非常有用的。在这种情况下，集群中某个节点出现故障时，会使得从各个父RDD计算出的数据丢失，造成需要重新计算。相反，对于那些窄依赖的RDD,对其进行检查点操作就不是有必须。在这种情况下如果一个节点发生故障，RDD在该节点中丢失的分区数据可以通过并行的方式从其他节点中重新计算出来，计算成本只是复制RDD的很小部分。</p>
<p>Spark提供为RDD设置检查点操作的API，可以让用户自行决定需要为那些数据设置检查点操作。另外由于RDD的只读特性，使得不需要关心数据一致性问题， 比常用的共享内存更容易做检查点。</p>
<h2 id="多用户管理"><a href="#多用户管理" class="headerlink" title="多用户管理"></a>多用户管理</h2><p>RDD模型将计算分解为多个相互独立的细粒度任务，这使得它在多用户集群能够支持多种资源共享算法。特别地，每个RDD应用可以在执行过程中动态调整访问资源。</p>
<ul>
<li>在每个应用程序中， Spark 运行多线程同时提交作业，并通过一种等级 公平调度器来实现多个作业对集群资源的共享，这种调度器和Hadoop Fair Scheduler 类似。该算法主要用于创建基于针对相同内存数据的多用户应用，例如: Spark SQL引擎有一个服务模式支持多用户并行查询。公平调度算法确保短的作业能够在即使长作业占满集群资源的情况下尽早完成。</li>
<li>Spark的公平调度也使用延迟调度,通过轮询每台机器的数据，在保持公平的情况下给予作业高的本地性。Spark 支持多级本地化访问策略(本地化)，包括内存、磁盘和机架。</li>
<li>由于任务相互独立，调度器还支持取消作业来为高优先级的作业腾出资源。</li>
<li>Spark中可以使用Mesos来实现细粒度的资源共享,这使得Spark应用能相互之间或在不同的计算框架之间实现资源的动态共享。</li>
<li>Spark使用Sparrow系统扩展支持分布式调度，该调度允许多个Spark应用以去中心化的方式在同一集群上排队工作，同时提供数据本地性、低延迟和公平性。</li>
</ul>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/Java大数据开发入门系列-四-————Spark之作业调度/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/Java大数据开发入门系列-四-————Spark之作业调度/" class="post-title-link" itemprop="url">Java大数据开发入门系列(四)————Spark之作业调度</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-09-07 22:18:18" itemprop="dateCreated datePublished" datetime="2020-09-07T22:18:18+08:00">2020-09-07</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-09-11 13:26:41" itemprop="dateModified" datetime="2020-09-11T13:26:41+08:00">2020-09-11</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Spark 有几种计算资源调度的方式。前面讲到，每个Spark应用（SparkContext的实例）都运行一组独立的<code>executor</code>进程。spark运行的集群管理器提供了跨应用之间的资源调度<a href="http://spark.apachecn.org/#/docs/23?id=scheduling-across-applications" target="_blank" rel="noopener">scheduling across applications</a>。其次，在每个Spark应用中，如果多个 <code>jobs</code>（Spark算子）是由不同的线程提交的，那么它们可能会并发运行。如果你的应用是常见的通过网络服务请求，Spark包含一个<a href="#应用内调度">fair scheduler</a>(公平的调度器)来调度每个SparkContext内的资源。在 Spark 应用内部（对应同一个 SparkContext）各个作业之间，Spark 默认 FIFO 调度，同时也可以支持公平调度。</p>
<h1 id="跨应用调度"><a href="#跨应用调度" class="headerlink" title="跨应用调度"></a>跨应用调度</h1><p>当在集群上运行时，每个Spark应用都会得到一组独立的<code>executor</code>JVM，来运行其任务并存储数据。如果多个用户需要共享你的集群，那么会有很多资源分配相关的选项，如何设计还取觉于具体的集群管理器。</p>
<p>对 Spark 所支持的各个集群管理器而言，最简单的的资源分配，就是静态划分。这种方式就意味着，每个 Spark 应用都是设定一个最大可用资源总量，并且该应用在整个生命周期内都会占住这个资源。这种方式在 Spark独立部署 standalone 和 YARN调度，以及 Mesos 粗粒度模式(coarse-grained Mesos mode)下都可用。资源分配可以根据集群类型进行如下配置：</p>
<ul>
<li><strong>Standalone mode:</strong> 默认情况下，提交到<code>standalone mode</code>集群的应用将以FIFO（先进先出）的顺序运行，并且每个spark应用都会占用集群中所有可用节点。不过你可以通过设置 spark.cores.max 或者 spark.deploy.defaultCores 来限制单个应用所占用的节点个数。最后，除了可以控制对 CPU 的使用数量之外，还可以通过spark.executor.memory来控制各个应用的内存占用量。</li>
<li><strong>Mesos:</strong> 在Mesos中要使用静态划分的话，需要将 spark.mesos.coarse 设为true，同样，你也需要配置 spark.cores.max来控制各个应用的 CPU 总数，以及 spark.executor.memory 来控制各个应用的内存占用。</li>
<li><strong>YARN:</strong> 在 YARN 中需要使用 –num-executors 选项来控制 Spark 应用在集群中分配的执行器的个数。对于单个执行器（executor）所占用的资源，可以使用 –executor-memory 和 –executor-cores 来控制。</li>
</ul>
<p>Mesos 上还有一种动态共享 CPU 的方式。在这种模式下，每个 Spark 应用的内存占用仍然是固定且独占的(仍由 spark.exexcutor.memory 决定)，但是如果该 Spark 应用没有在某个机器上执行任务的话，那么其它应用可以占用该机器上的 CPU。这种模式对集群中有大量不是很活跃应用的场景非常有效，例如：集群中有很多不同用户的 Spark shell session。但这种模式不适用于低延时的场景，因为当 Spark 应用需要使用 CPU 的时候，可能需要等待一段时间才能取得对 CPU 的使用权。要使用这种模式，只需要在 mesos://URL 上设置 spark.mesos.coarse 属性为 false 即可。</p>
<p>值得注意的是，目前还没有任何一种资源分配模式支持跨 Spark 应用的内存共享。如果你想通过这种方式共享数据，我们建议你可以单独使用一个服务(例如：alluxio)，这样就能实现多应用访问同一个 RDD 的数据。</p>
<h2 id="动态资源分配"><a href="#动态资源分配" class="headerlink" title="动态资源分配"></a>动态资源分配</h2><p>Spark 提供了一种基于负载来动态调节 Spark 应用资源占用的机制。这意味着，你的应用会在资源空闲的时间将其释放给集群，需要时再重新申请。这一特性在多个应用 Spark 集群资源的情况下特别有用。</p>
<p>这个特性默认是禁止的，但是在所有的粗粒度集群管理器上都是可用的，如：独立部署模式<code>standalone mode</code>，<code>YARN mode</code>，和<code>Mesos 粗粒度模式</code>(coarse-grained Mesos mode)。</p>
<h3 id="配置和部署"><a href="#配置和部署" class="headerlink" title="配置和部署"></a>配置和部署</h3><p>要使用动态资源分配这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个worker节点上启动 external shuffle service，同时将 spark.shuffle.service.enabled 设为 true。external shuffle service 的目的是在移除 executor 的时候，能够保留 executor 输出的 shuffle 文件。启用 external shuffle service 的方式在各个集群管理器上各不相同：</p>
<p>在 Spark 独立部署的集群中，你只需要在 worker 启动前设置 spark.shuffle.service.enabled 为 true 即可。</p>
<p>在 Mesos 粗粒度模式下，你需要在各个节点上运行 $SPARK_HOME/sbin/start-mesos-shuffle-service.sh 并设置 spark.shuffle.service.enabled为true即可。例如，你可以在Marathon来启用这一功能。</p>
<p>在YARN模式下，需要按以下步骤在各个 NodeManager 上启动：<a href="#YARN启动shuffle服务">here</a></p>
<h3 id="资源分配策略"><a href="#资源分配策略" class="headerlink" title="资源分配策略"></a>资源分配策略</h3><p>总体上来说，Spark 应该在executor空闲时将其关闭，而在后续要用时再申请。因为没有一个固定的方法，可以预测一个executor在后续是否马上会被分配去执行任务，或者一个新分配的executor实际上是空闲的，所以我们需要一个试探性的方法，来决定是否申请或是移除一个executor。</p>
<h4 id="请求策略"><a href="#请求策略" class="headerlink" title="请求策略"></a>请求策略</h4><p>一个启用了动态分配的 Spark 应用在等待任务需要调度的时候，会去申请额外的executor。在这种情况下，必定意味着已有的executor已经不足以同时执行所有未完成的任务。</p>
<p>Spark会轮流来申请executor。当有待处理的任务达到spark.dynamicAllocation.scheduleerBacklogTimeout秒时，就会触发实际的请求，如果等待队列中仍有挂起的任务，则每过 spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 秒后触发一次资源申请。另外，每一轮申请的executor个数以指数形式增长。例如：一个 Spark 应用可能在首轮申请 1 个执行器，后续的轮次申请个数可能是 2 个、4 个、8 个……。</p>
<p>采用指数级增长策略的原因有两个：第一，一个应用程序在开始时应该谨慎地请求executor，以防万一发现只增加几个executor就够了，这和 TCP 慢启动有些类似；第二，如果一旦 Spark 应用确实需要申请多个executor的话，那么可以确保其所需的计算资源及时增长。</p>
<h4 id="移除策略"><a href="#移除策略" class="headerlink" title="移除策略"></a>移除策略</h4><p>移除executor的策略就简单得多了。Spark 应用会在某个执行器空闲超过 spark.dynamicAllocation.executorIdleTimeout 秒后将其删除，在大多数情况下，执行器的移除条件和申请条件都是互斥的，也就是说，执行器在有等待执行任务挂起时，不应该空闲。</p>
<h3 id="优雅的关闭Executor"><a href="#优雅的关闭Executor" class="headerlink" title="优雅的关闭Executor"></a>优雅的关闭Executor</h3><p>在非动态分配下，executor可能的退出原因有执行失败或是相关 Spark 应用已经退出。不管是哪种原因，executor的所有相关联的状态都已经不再需要，可以丢弃掉。但是在动态分配的情况下，executor有可能在 Spark 应用运行期间被移除。这时候，如果 Spark 应用尝试去访问该executor存储的状态，就必须重算这一部分数据。因此，Spark 需要一种机制，能够优雅的关闭executor，同时还保留其状态数据。</p>
<p>这种需求对于shuffles操作尤其重要。shuffle过程中，Spark executor首先将 map 输出写到本地磁盘，然后在其他executor试图获取这些map结果数据时，充当这些文件的服务器。一旦有某些任务执行时间过长，动态分配有可能在shuffle结束前移除任务异常的executor，而这些被移除的executor对应的数据将会被重新计算，但这些重算其实是不必要的。</p>
<p>要解决这一问题，就需要用到 external shuffle service，该服务在 Spark 1.2 引入。该服务在每个节点上都会启动一个不依赖于任何 Spark 应用或executor的独立进程。一旦该服务启用，Spark executor不再从各个executor上获取 shuffle文件，转而从这个 service 获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。</p>
<p>除了shuffle文件之外，executor也会在磁盘或者内存中缓存数据。一旦executor被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会类似采用external shuffle service的方法，将缓存数据保存在堆外存储中以解决这一问题。</p>
<h1 id="应用内调度"><a href="#应用内调度" class="headerlink" title="应用内调度"></a>应用内调度</h1><p>在指定的 Spark 应用内部（对应同一 SparkContext 实例），多个线程可能并发地提交 Spark 作业（job），他们可以同时运行。在本节中，作业（job）是指，由 Spark action 算子（如：collect）触发的一系列计算任务的集合。Spark 调度器是完全线程安全的，并且能够支持 Spark 应用同时处理多个请求（比如：来自不同用户的查询）。</p>
<p>默认，Spark 应用内部使用 FIFO 调度策略。每个作业被划分为多个阶段（stage）（例如：map 阶段和 reduce 阶段），第一个作业在其启动后会优先获取所有的可用资源，然后是第二个作业再申请，再第三个……。如果前面的作业没有把集群资源占满，则后续的作业可以立即启动运行，否则，后提交的作业会有明显的延迟等待。</p>
<p>不过从 Spark 0.8 开始，Spark 也能支持各个作业间的公平（Fair）调度。公平调度时，Spark 以轮询的方式给每个作业分配资源，因此所有的作业获得的资源大体上是平均分配。这意味着，即使有大作业在运行，小的作业再提交也能立即获得计算资源而不是等待前面的作业结束，大大减少了延迟时间。这种模式特别适合于多用户配置。要启用公平调度器，只需设置一下 SparkContext 中 spark.scheduler.mode 属性为 FAIR 即可 :</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line">conf.set(<span class="string">"spark.scheduler.mode"</span>, <span class="string">"FAIR"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>

<h2 id="公平调度资源池"><a href="#公平调度资源池" class="headerlink" title="公平调度资源池"></a>公平调度资源池</h2><p>公平调度器还可以支持将作业分组放入资源池（pool），然后给每个资源池配置不同的调度选项（如：权重）(ps:就是设置调度优先级)。这样你就可以给一些比较重要的作业创建一个“高优先级”资源池，或者你也可以把每个用户的作业分到一组，这样一来就是各个用户平均分享集群资源，而不是各个作业平分集群资源。Spark 公平调度的实现方式基本都是模仿 <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">Hadoop Fair Scheduler</a>来实现的。</p>
<p>默认情况下，新提交的作业都会进入到默认资源池中，不过作业对应于哪个资源池，可以在提交作业的线程中用 SparkContext.setLocalProperty 设定 spark.scheduler.pool 属性。示例代码如下 :</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assuming sc is your SparkContext variable</span></span><br><span class="line">sc.setLocalProperty(<span class="string">"spark.scheduler.pool"</span>, <span class="string">"pool1"</span>)</span><br></pre></td></tr></table></figure>

<p>一旦设好了局部属性，所有该线程所提交的作业（即：在该线程中调用 action 算子，如：RDD.save/count/collect 等）都会使用这个资源池。这个设置是以线程为单位保存的，你很容易实现用同一线程来提交同一用户的所有作业到同一个资源池中。同样，如果需要清除资源池设置，只需在对应线程中调用如下代码 :</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.setLocalProperty(<span class="string">"spark.scheduler.pool"</span>, <span class="literal">null</span>)</span><br></pre></td></tr></table></figure>

<h2 id="资源池默认行为"><a href="#资源池默认行为" class="headerlink" title="资源池默认行为"></a>资源池默认行为</h2><p>默认地，各个资源池之间平分整个集群的资源（包括 default 资源池），但在资源池内部，默认情况下，作业是 FIFO 顺序执行的。举例来说，如果你为每个用户创建了一个资源池，那么久意味着各个用户之间共享整个集群的资源，但每个用户自己提交的作业是按顺序执行的，而不会出现后提交的作业抢占前面作业的资源。</p>
<h2 id="配置资源池属性"><a href="#配置资源池属性" class="headerlink" title="配置资源池属性"></a>配置资源池属性</h2><p>资源池的属性需要通过配置文件来指定。每个资源池都支持以下3个属性 :</p>
<ul>
<li><code>schedulingMode</code>：可以是 FIFO 或 FAIR，控制资源池内部的作业是如何调度的。</li>
<li><code>weight</code>：控制资源池相对其他资源池，可以分配到资源的比例。默认所有资源池的 weight 都是 1。如果你将某个资源池的 weight 设为 2，那么该资源池中的资源将是其他池子的2倍。如果将 weight 设得很高，如 1000，可以实现资源池之间的调度优先级 – 也就是说，weight=1000 的资源池总能立即启动其对应的作业。</li>
<li><code>minShare</code>：除了整体 weight 之外，每个资源池还能指定一个最小资源分配值（CPU 个数），管理员可能会需要这个设置。公平调度器总是会尝试优先满足所有活跃（active）资源池的最小资源分配值，然后再根据各个池子的 weight 来分配剩下的资源。因此，minShare 属性能够确保每个资源池都能至少获得一定量的集群资源。minShare 的默认值是 0。</li>
</ul>
<p>资源池属性是一个 XML 文件，可以基于 conf/fairscheduler.xml.template 修改，然后在 <a href="http://spark.apachecn.org/#/configuration.html?id=spark-properties" target="_blank" rel="noopener">SparkConf</a>。的 spark.scheduler.allocation.file 属性指定文件路径：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(<span class="string">"spark.scheduler.allocation.file"</span>, <span class="string">"/path/to/file"</span>)</span><br></pre></td></tr></table></figure>

<p>资源池 XML 配置文件格式如下，其中每个池子对应一个 <pool>元素，每个资源池可以有其独立的配置 :</pool></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"production"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FAIR<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>2<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"test"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FIFO<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>2<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>3<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="额外补充："><a href="#额外补充：" class="headerlink" title="额外补充："></a>额外补充：</h1><h2 id="Shuffle相关介绍"><a href="#Shuffle相关介绍" class="headerlink" title="Shuffle相关介绍"></a>Shuffle相关介绍</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>有些运算需要将各节点上的同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则汇集到一起的过程称为 Shuffle。</p>
<p><img src="http://img.rogermaster.top/uPic/4C16KL.jpg" alt="4C16KL"></p>
<p>在MapReduce框架，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce，而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I/O。所以shuffle性能的高低也直接决定了整个程序的性能高低。而Spark也会有自己的shuffle实现过程。</p>
<h3 id="Spark中的-shuffle"><a href="#Spark中的-shuffle" class="headerlink" title="Spark中的 shuffle"></a>Spark中的 shuffle</h3><p>在DAG调度的过程中，<strong>Stage</strong> 阶段的划分是根据是否有shuffle过程，也就是存在 <strong>宽依赖</strong> 的时候,需要进行shuffle,这时候会将 <strong>job</strong> 划分成多个Stage，每一个 Stage 内部有很多可以并行运行的 <strong>Task</strong>。</p>
<p><img src="http://img.rogermaster.top/uPic/XsfPkt.png" alt="XsfPkt"></p>
<p>标注对应的RDD后：</p>
<p>![image-20200908223941532](/Users/roger/Library/Application Support/typora-user-images/image-20200908223941532.png)</p>
<p><img src="http://img.rogermaster.top/uPic/LEZncX.png" alt="LEZncX"></p>
<p>stage与stage之间的过程就是 shuffle 阶段，前一个 Stage 的 ShuffleMapTask 进行 Shuffle Write， 把数据存储在 BlockManager 上面， 并且把数据位置元信息上报到 Driver 的 MapOutTrack 组件中， 下一个 Stage 根据数据位置元信息， 进行 Shuffle Read， 拉取上个 Stage 的输出数据。在 Spark 中，负责 shuffle 过程的执行、计算和处理的组件主要就是 <strong>ShuffleManager</strong> 。ShuffleManager 随着Spark的发展有两种实现的方式，分别为 <strong>HashShuffleManager</strong> 和 <strong>SortShuffleManager</strong> ，因此spark的Shuffle有 <strong>Hash Shuffle</strong>  和 <strong>Sort Shuffle</strong> 两种。</p>
<p>在 <strong>Spark 1.2</strong> 以前，默认的shuffle计算引擎是 <strong>HashShuffleManager</strong> 。</p>
<p><strong>HashShuffleManager</strong> 有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。因此在Spark 1.2以后的版本中，默认的 <strong>ShuffleManager</strong> 改成了 <strong>SortShuffleManager</strong> 。</p>
<p><strong>SortShuffleManager</strong> 相较于 HashShuffleManager 来说，有了一定的改进。主要就在于每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并(merge)成一个磁盘文件，因此每个 Task 就只有一个磁盘文件。在下一个 Stage 的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p>
<p>Hash shuffle是不具有排序的Shuffle。</p>
<h2 id="YARN启动shuffle服务"><a href="#YARN启动shuffle服务" class="headerlink" title="YARN启动shuffle服务"></a>YARN启动shuffle服务</h2><p>要在YARN群集中的每个NodeManager上启动Spark Shuffle服务，请遵循以下说明：</p>
<ol>
<li>用YARN配置文件构建Spark。如果你使用的是预打包的发行版，请跳过这一步。</li>
<li>找到<code>spark-&lt;version&gt;-yarn-shuffle.jar</code>. 如果你是自己构建的spark会在 <code>$SPARK_HOME/common/network-yarn/target/scala-&lt;version&gt;</code> 下,如果你用的我是发行版则在 <code>yarn</code> 下.</li>
<li>将这个jar包添加到集群中所有NodeManagers的classpath中。</li>
<li>在每个节点的yarn-site.xml中，将spark_shuffle添加到yarn.nodemanager.aux-services中，然后将yarn.nodemanager.aux-services.spark_shuffle.class设置org.apache.spark.network.yarn.YarnShuffleService。</li>
<li>在 etc/hadoop/yarn-env.sh 中设置 YARN_HEAPSIZE（默认为 1000），增加 NodeManager 的堆大小，以避免在shuffle中的垃圾回收问题。</li>
<li>重新启动集群中的所有NodeManagers。</li>
</ol>
<p>在YARN上运行shuffle service时，可以使用以下附加配置选项：</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.yarn.shuffle.stopOnFailure</code></td>
<td align="left"><code>false</code></td>
<td align="left">当Spark shuffle service的初始化出现故障时，是否要停止NodeManager。这可以防止在Spark shuffle service没有运行的NodeManagers上运行容器导致的应用失败。</td>
</tr>
</tbody></table>
<h2 id="Spark调度算法-FIFO、FAIR"><a href="#Spark调度算法-FIFO、FAIR" class="headerlink" title="Spark调度算法(FIFO、FAIR)"></a>Spark调度算法(FIFO、FAIR)</h2><p>FIFO模式的算法类是FIFOSchedulingAlgorithm，FAIR模式的算法实现类是FairSchedulingAlgorithm。下面看两种模式下的比较函数的实现，FIFO：</p>
<p>先比较priority，在FIFO中该优先级实际上是Job ID，越早提交的job的jobId越小，priority越小，优先级越高。</p>
<p>若priority相同，则说明是同一个job里的TaskSetMagager，则比较StageId，StageId越小优先级越高。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> priority1 = s1.priority</span><br><span class="line">    <span class="keyword">val</span> priority2 = s2.priority</span><br><span class="line">    <span class="keyword">var</span> res = math.signum(priority1 - priority2)</span><br><span class="line">    <span class="comment">//优先级比较，越小，就最先执行</span></span><br><span class="line">    <span class="keyword">if</span> (res == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> stageId1 = s1.stageId</span><br><span class="line">      <span class="keyword">val</span> stageId2 = s2.stageId</span><br><span class="line">      res = math.signum(stageId1 - stageId2)</span><br><span class="line">      <span class="comment">//优先级相同，就比较stageId，StageId越小优先级越高</span></span><br><span class="line">    &#125;</span><br><span class="line">    res &lt; <span class="number">0</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.rogermaster.top/uPic/2kctUu.jpg" alt="2kctUu"></p>
<p>FAIR 模式中有一个 rootPool 和多个子 Pool， 各个子 Pool 中存储着所有待分配的 <code>TaskSetMagager</code> 。</p>
<p>在  FAIR  模 式 中 ， 需 要 先 对 子  Pool 进 行 排 序 ， 再 对 子  Pool 里 面 的</p>
<p><code>TaskSetMagager</code>进行排序，因为<code>Pool</code>和<code>TaskSetMagager</code>都继承了Schedulable特质， 因此使用相同的排序算法。</p>
<p>排序过程的比较是基于 <code>Fair-share</code> 来比较的，每个要排序的对象包含三个属性:</p>
<p>runningTasks 值（ 正在运行的 Task 数）、minShare 值、weight 值，比较时会综合考量 runningTasks 值， minShare 值以及 weight 值。</p>
<p>注意，minShare、weight 的值均在公平调度配置文件 fairscheduler.xml 中被指定，调度池在构建阶段会读取此文件的相关配置。</p>
<p>1)    如果 A 对象的 runningTasks 大于它的 minShare， B 对象的 runningTasks 小于它的 minShare，那么 B 排在 A 前面； （ 如果一个<code>Pool</code>的<code>miniShare</code>够用，另一个不够用，先分配给够用的。）</p>
<p>2)    如果 A 、B 对象的 runningTasks 都小于它们的 minShare ，那么就比较runningTasks 与 minShare 的比值（ <code>minShare</code> 使用率），谁小谁排前面；（ 两个<code>poll</code>都够用，谁占<code>miniShare</code>的少先分配给谁。例如两个Pool同样数量的<code>runningTask</code>，先分配给<code>miniShare</code>大的。）</p>
<p>3)    如果 A 、B 对象的 <code>runningTasks</code> 都大于它们的 <code>minShare</code> ，那么就比较<code>runningTasks</code> 与 <code>weight</code> 的比值（ 权重使用率），谁小谁排前面。（同样数量的<code>runningTask</code>，先分配给<code>weight</code>大的）</p>
<p>4)    如果上述比较均相等，则比较名字。</p>
<p>整体上来说就是通过 <code>minShare</code> 和 <code>weight</code> 这两个参数控制比较过程， 可以做到让 <code>minShare</code> 使用率和权重使用率少（ 实际运行 task 比例较少） 的先运行。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">FairSchedulingAlgorithm</span> <span class="keyword">extends</span> <span class="title">SchedulingAlgorithm</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> minShare1 = s1.minShare</span><br><span class="line">    <span class="keyword">val</span> minShare2 = s2.minShare</span><br><span class="line">    <span class="comment">//默认为0,除非通过fair的配置文件进行了配置指定</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> runningTasks1 = s1.runningTasks</span><br><span class="line">    <span class="keyword">val</span> runningTasks2 = s2.runningTasks</span><br><span class="line">    <span class="comment">/* 如果是TaskSetManager时,就是taskSet中运行的task的个数,</span></span><br><span class="line"><span class="comment">     * 如果是Pool实例是表示是所有使用这个poolName的所有的TaskSetManager正在运行的task的个数.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> s1Needy = runningTasks1 &lt; minShare1</span><br><span class="line">    <span class="keyword">val</span> s2Needy = runningTasks2 &lt; minShare2</span><br><span class="line">    <span class="comment">//只有在minShare在fair的配置文件中显示配置,同时大于正在运行的task的个数时,才会为true</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> minShareRatio1 = runningTasks1.toDouble / math.max(minShare1, <span class="number">1.0</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> minShareRatio2 = runningTasks2.toDouble / math.max(minShare2, <span class="number">1.0</span>).toDouble</span><br><span class="line">    <span class="comment">//运行的task的个数针对于minShare的比重</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> taskToWeightRatio1 = runningTasks1.toDouble / s1.weight.toDouble</span><br><span class="line">    <span class="keyword">val</span> taskToWeightRatio2 = runningTasks2.toDouble / s2.weight.toDouble</span><br><span class="line">    <span class="comment">//得到正在运行的task个数针对于pool的weight的比重</span></span><br><span class="line">    <span class="keyword">var</span> compare: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">/*这里首先根据正在运行的task的个数是否已经达到调度队列中最小的分片的个数来进行排序,</span></span><br><span class="line"><span class="comment">    * 如果s1中运行运行的个数小于s1的pool的配置的minShare,返回true,表示s1排序在前面.</span></span><br><span class="line"><span class="comment">    * 如果s2中运行的task的个数小于s2的pool中配置的minShare(最小分片数)的值,表示s1小于s2,这时s2排序应该靠		 * 前.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> (s1Needy &amp;&amp; !s2Needy) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">    <span class="comment">/*这种情况表示s1与s2两个队列中,正在运行的task的个数都已经大于(不小于)了两个子调度器中配置的minShare的   		* 个数时,根据两个子调度器队列中正在运行的task的个数对应此调度器中最小分片的值所占的比重最小的一个排序更靠前</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">      compare = minShareRatio1.compareTo(minShareRatio2)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">/*这种情况表示s1与s2两个子调度器的队列中,正在运行的task的个数都还没有达到配置的最小分片的个数的情况,比</span></span><br><span class="line"><span class="comment">    *	较两个队列中正在运行的task的个数对应调度器队列的weigth的占比,最小的一个排序更靠前</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">      compare = taskToWeightRatio1.compareTo(taskToWeightRatio2)</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> (compare &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (compare &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">//如果两个根据上面的计算,排序值都相同,就看看这两个调度器的名称,按名称的字节序来排序了.</span></span><br><span class="line">      s1.name &lt; s2.name</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参考文献：</p>
<ul>
<li><a href="https://juejin.im/post/6844904065218920456#heading-21" target="_blank" rel="noopener">Spark的Shuffle总结分析</a>-说出你的愿望吧</li>
<li><a href="https://zhuanlan.zhihu.com/p/67061627" target="_blank" rel="noopener">Spark Shuffle 详解</a>-小鹏</li>
<li><a href="https://spark.apache.org/docs/latest/job-scheduling.html#job-scheduling" target="_blank" rel="noopener">Job-scheduling</a>-apache</li>
<li><a href="https://www.cnblogs.com/hanhaotian/p/11791966.html" target="_blank" rel="noopener">SPARK的TASK调度器(FAIR公平调度算法)</a>-herman很慢</li>
</ul>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/Java大数据开发入门系列-四-————Spark简介/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/Java大数据开发入门系列-四-————Spark简介/" class="post-title-link" itemprop="url">Java大数据开发入门系列(四)————Spark简介</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-09-02 12:52:34" itemprop="dateCreated datePublished" datetime="2020-09-02T12:52:34+08:00">2020-09-02</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-09-14 23:07:14" itemprop="dateModified" datetime="2020-09-14T23:07:14+08:00">2020-09-14</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Spark 于 2009 年诞生于加州大学伯克利分校 AMPLab，2013 年被捐赠给 Apache 软件基金会，2014 年 2 月成为 Apache 的顶级项目。相对于 MapReduce 的批处理计算，Spark 可以带来上百倍的性能提升，因此它成为继 MapReduce 之后，最为广泛使用的分布式计算框架。</p>
<h2 id="二、Spark产生背景"><a href="#二、Spark产生背景" class="headerlink" title="二、Spark产生背景"></a>二、Spark产生背景</h2><p>Spark 是在 MapReduce 的基础上产生的，借鉴了大量 MapReduce 实践经验，引入多种新型涉及思想和优化策略。针对MapReduce计算框架存在的局限性进行分析，能更好的了解到 Spark。</p>
<p>MapReduce 的局限性如下:</p>
<p>1、处理效率低效</p>
<ul>
<li>Map中间结果写磁盘，Reduce写HDFS，多个MR之间通过HDFS交换数据</li>
<li>任务调度和启动开销大</li>
<li>无法充分利用内存</li>
<li>Map 端和 Reduce 端均需要排序</li>
<li>复杂功能 Io 开销大，对于复杂 sql，需转换 MapReduce 计算，需要通过 HDFS 进行磁盘数据交换，而读写Hfds需消耗大量磁盘和网络 IO</li>
</ul>
<p>2、 不适合迭代计算（如机器学习、图计算等），交互式处理（数据挖掘） 和流式处理（点击日志分析）<br>3、 MapReduce 编程不够灵活</p>
<ul>
<li>仅支持 Map 和 Reduce 两种操作</li>
<li>尝试函数式编程风格</li>
</ul>
<p>4、计算框架多样化、无形中产生运维和管理成本</p>
<h2 id="三、特点"><a href="#三、特点" class="headerlink" title="三、特点"></a>三、特点</h2><p>Spark 是基与 MapReduce 基础产生了，克服了其存在的性能低下，变成不够灵活的缺点。</p>
<p>Spark 作为一种 DAG 计算框架，主要特点如下:</p>
<p>Apache Spark 具有以下特点：</p>
<ul>
<li>使用先进的 DAG 调度程序，查询优化器和物理执行引擎，以实现性能上的保证；</li>
<li>多语言支持，目前支持的有 Java，Scala，Python 和 R；</li>
<li>提供了 80 多个高级 API，可以轻松地构建应用程序；</li>
<li>支持批处理，流处理和复杂的业务分析；</li>
<li>丰富的类库支持：包括 SQL，MLlib，GraphX 和 Spark Streaming 等库，并且可以将它们无缝地进行组合；</li>
<li>丰富的部署模式：支持本地模式和自带的集群模式，也支持在 Hadoop，Mesos，Kubernetes 上运行；</li>
<li>多数据源支持：支持访问 HDFS，Alluxio，Cassandra，HBase，Hive 以及数百个其他数据源中的数据。</li>
</ul>
<p><img src="http://img.rogermaster.top/uPic/r9I3DA.jpg" alt="r9I3DA"></p>
<h2 id="三、Spark的组成"><a href="#三、Spark的组成" class="headerlink" title="三、Spark的组成"></a>三、Spark的组成</h2><p>Spark组成(BDAS)：全称伯克利数据分析栈，通过大规模集成算法、机器、人之间展现大数据应用的一个平台。也是处理大数据、云计算、通信的技术解决方案。</p>
<p>它的主要组件有：</p>
<ul>
<li>SparkCore：将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。</li>
<li>SparkSQL：Spark Sql 是 Spark 来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。</li>
<li>SparkStreaming：是 Spark 提供的实时数据进行流式计算的组件。</li>
<li>MLlib：提供常用机器学习算法的实现库。</li>
<li>GraphX：提供一个分布式图计算框架，能高效进行图计算。</li>
<li>BlinkDB：用于在海量数据上进行交互式SQL的近似查询引擎。</li>
<li>Tachyon：以内存为中心高容错的的分布式文件系统。</li>
</ul>
<h2 id="四、集群架构"><a href="#四、集群架构" class="headerlink" title="四、集群架构"></a>四、集群架构</h2><table>
<thead>
<tr>
<th>Term（术语）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>Spark 应用程序，由集群上的一个 Driver 节点和多个 Executor 节点组成。</td>
</tr>
<tr>
<td>Driver program</td>
<td>主运用程序，该进程运行 main() 方法并且创建 SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>集群资源管理器（例如，Standlone Manager，Mesos，YARN）</td>
</tr>
<tr>
<td>Worker node</td>
<td>执行计算任务的工作节点</td>
</tr>
<tr>
<td>Executor</td>
<td>位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中</td>
</tr>
<tr>
<td>Task</td>
<td>被发送到 Executor 中的工作单元</td>
</tr>
<tr>
<td>Job</td>
<td>一个Action算子（比如collect算子）对应一个Job，由并行计算的多个Task组成。</td>
</tr>
<tr>
<td>Stage</td>
<td>每个Job由多个Stage组成，每个Stage是一个Task集合，由DAG分割而成。</td>
</tr>
<tr>
<td>Task</td>
<td>承载业务逻辑的运算单元，是Spark平台中可执行的最小工作单元。一个应用根据执行计划以及计算量分为多个Task。</td>
</tr>
</tbody></table>
<p>Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行。</p>
<p><a href="https://camo.githubusercontent.com/ab723935b3ac3fb5e85e12483af3aa81da1686e7/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f737061726b2de99b86e7bea4e6a8a1e5bc8f2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/ab723935b3ac3fb5e85e12483af3aa81da1686e7/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f737061726b2de99b86e7bea4e6a8a1e5bc8f2e706e67" alt="img"></a></p>
<p><strong>执行过程</strong>：</p>
<ol>
<li>用户程序创建 SparkContext 后，它会连接到集群资源管理器，集群资源管理器会为用户程序分配计算资源，并启动 Executor；</li>
<li>Driver 将计算程序划分为不同的执行阶段和多个 <code>task</code>，之后将 <code>task</code> 发送给 Executor；</li>
<li>Executor 负责执行 Task，并将执行状态汇报给 Driver，同时也会将当前节点资源的使用情况汇报给集群资源管理器。</li>
<li>因为Driver调度了集群上的 <code>task</code>（任务），更好的方式应该是在相同的局域网中靠近 <code>worker</code> 的节点上运行。如果你不喜欢发送请求到远程的集群，倒不如打开一个 <code>RPC</code> 至 <code>driver</code> 并让它就近提交操作而不是从很远的 <code>worker</code> 节点上运行一个 <code>driver</code>。</li>
</ol>
<p><strong>driver做什么</strong></p>
<ul>
<li>运行应用程序的main函数</li>
<li>创建spark的上下文</li>
<li>划分RDD并生成有向无环图（DAGScheduler）</li>
<li>与spark中的其他组进行协调，协调资源等等（SchedulerBackend）</li>
<li>生成并发送task到executor（taskScheduler）</li>
</ul>
<h2 id="五、核心组件"><a href="#五、核心组件" class="headerlink" title="五、核心组件"></a>五、核心组件</h2><p>Spark 基于 Spark Core 扩展了四个核心组件，分别用于满足不同领域的计算需求。</p>
<p><img src="http://img.rogermaster.top/uPic/0HsQfr.jpg" alt="0HsQfr"></p>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>Spark SQL 主要用于结构化数据的处理。其具有以下特点：</p>
<ul>
<li>能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询；</li>
<li>支持多种数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC；</li>
<li>支持 HiveQL 语法以及用户自定义函数 (UDF)，允许你访问现有的 Hive 仓库；</li>
<li>支持标准的 JDBC 和 ODBC 连接；</li>
<li>支持优化器，列式存储和代码生成等特性，以提高查询效率。</li>
</ul>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>Spark Streaming 主要用于快速构建可扩展，高吞吐量，高容错的流处理程序。支持从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，并进行处理。</p>
<p><img src="http://img.rogermaster.top/uPic/zcaFjF.jpg" alt="zcaFjF"></p>
<p>Spark Streaming 的本质是微批处理，它将数据流进行极小粒度的拆分，拆分为多个批处理，从而达到接近于流处理的效果。</p>
<p><img src="http://img.rogermaster.top/uPic/JUE5Rf.jpg" alt="JUE5Rf"></p>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><p>MLlib 是 Spark 的机器学习库。其设计目标是使得机器学习变得简单且可扩展。它提供了以下工具：</p>
<ul>
<li><strong>常见的机器学习算法</strong>：如分类，回归，聚类和协同过滤；</li>
<li><strong>特征化</strong>：特征提取，转换，降维和选择；</li>
<li><strong>管道</strong>：用于构建，评估和调整 ML 管道的工具；</li>
<li><strong>持久性</strong>：保存和加载算法，模型，管道数据；</li>
<li><strong>实用工具</strong>：线性代数，统计，数据处理等。</li>
</ul>
<h3 id="Graphx"><a href="#Graphx" class="headerlink" title="Graphx"></a>Graphx</h3><p>GraphX 是 Spark 中用于图形计算和图形并行计算的新组件。在高层次上，GraphX 通过引入一个新的图形抽象来扩展 RDD(一种具有附加到每个顶点和边缘的属性的定向多重图形)。为了支持图计算，GraphX 提供了一组基本运算符（如： subgraph，joinVertices 和 aggregateMessages）以及优化后的 Pregel API。此外，GraphX 还包括越来越多的图形算法和构建器，以简化图形分析任务。</p>
<h2 id="六、spark作业运行流程"><a href="#六、spark作业运行流程" class="headerlink" title="六、spark作业运行流程"></a>六、spark作业运行流程</h2><p>Spark 有3 种运行模式，包括 Standalone、YARN 和 Mesos，其中，Mesos 和 YARN模式类似。目前用得比较多的是 Standalone 模式和 YARN 模式。下面将详细介绍 Standalone模式和 YARN 模式的启动方式及运行流程。</p>
<h3 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h3><p>Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和Worker节点。Driver既可以运行在Master节点上，也可以运行在本地Client端。</p>
<p>当以 Standalone 模式向 Spark 集群提交作业时，作业的运行流程如图所示。</p>
<p><img src="http://img.rogermaster.top/uPic/YSmAE8.png" alt="YSmAE8"></p>
<p>（1）首先，SparkContext 连接到 Master，向 Master 注册并申请资源。</p>
<p>（2）Worker 定期发送心跳信息给 Master 并报告 Executor 状态。</p>
<p>（3）Master 根据 SparkContext 的资源申请要求和 Worker 心跳周期内报告的信息决定在哪个 Worker 上分配资源,然后在该 Worker 上获取资源,启动 StandaloneExecutorBackend。</p>
<p>（4）StandaloneExecutorBackend向 SparkContext注册。</p>
<p>（5）SparkContext 将 Application 代码发送给 StandaloneExecutorBackend，并且 SparkContext 解析 Application 代码，构建 DAG 图，并提交给 DAG Scheduler，分解成 Stage（当碰到 Action 操作时，就会催生 Job，每个 Job 中含有一个或多个 Stage)，然后将 Stage（或者称为 TaskSet）提交给 Task Scheduler，Task Scheduler 负责将Task 分配到相应的 Worker,最后提交给 StandaloneExecutorBackend 执行。</p>
<p>（6）StandaloneExecutorBackend 会建立 Executor 线程池，开始执行 Task，并向 SparkContext 报告，直至 Task 完成。</p>
<p>（7）所有 Task 完成后，SparkContext 向 Master 注销，释放资源。</p>
<h3 id="Standalone-Client（Driver在client运行）"><a href="#Standalone-Client（Driver在client运行）" class="headerlink" title="Standalone-Client（Driver在client运行）"></a>Standalone-Client（Driver在client运行）</h3><p>提交命令：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  --master <span class="symbol">spark:</span>/<span class="regexp">/node001:7077,node002:7077 --deploy-mode client --class org.apache.spark.examples.SparkPi ../examples</span><span class="regexp">/jars/spark</span> examples_2.<span class="number">11</span>-<span class="number">2.3</span>.<span class="number">1</span>.jar  <span class="number">10000</span></span><br></pre></td></tr></table></figure>

<p><img src="http://img.rogermaster.top/uPic/otclL4.jpg" alt="otclL4"></p>
<p> （1）Client模式下提交任务，在客户端启动Driver进程。<br> （2）Driver会向Master申请启动Application启动的资源。<br> （3）资源申请成功，Driver端将Task发送到Worker端执行。<br> （4）Worker将Task执行结果返回到Driver端。</p>
<p>client模式适用于测试调试程序。Driver进程是在客户端启动的，这里的客户端就是指提交应用程序的当前节点。在Driver端可以看到task执行的情况。生产环境下不能使用client模式，因为Driver可能会回收task执行结果数据，假设要提交100个application到集群运行，Driver每次都会在client端启动，那么就会导致客户端所在节点的Driver收集100个application的结果数据，导致100次网卡流量暴增的问题。</p>
<h3 id="Standalone-Cluster（Driver在Worker运行）"><a href="#Standalone-Cluster（Driver在Worker运行）" class="headerlink" title="Standalone-Cluster（Driver在Worker运行）"></a>Standalone-Cluster（Driver在Worker运行）</h3><p>提交命令：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  --master <span class="symbol">spark:</span>/<span class="regexp">/node001:7077,node002:7077 --deploy-mode cluster --class</span></span><br></pre></td></tr></table></figure>

<p><img src="http://img.rogermaster.top/uPic/FvfZNe.jpg" alt="FvfZNe"></p>
<p>（1）Standalone-Cluster模式提交App后，会向Master请求启动Driver。<br>（2）Master接收请求之后，随机在集群中一台节点启动Driver进程。<br>（3）Driver启动后为当前的应用程序申请资源。<br>（4）Driver端发送task到worker节点上执行。<br>（5）worker将执行情况和执行结果返回给Driver端。</p>
<p>Driver进程是在集群某一台Worker上启动的，<strong>在客户端是无法查看task的执行情况的</strong>。假设要提交100个application到集群运行,<strong>每次Driver会随机在集群中某一台Worker上启动</strong>，那么这100次网卡流量暴增的问题就散布在集群上。</p>
<h3 id="Yarn模式"><a href="#Yarn模式" class="headerlink" title="Yarn模式"></a>Yarn模式</h3><h4 id="Yarn-cluster"><a href="#Yarn-cluster" class="headerlink" title="Yarn-cluster"></a>Yarn-cluster</h4><p>提交命令：</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  --master yarn --deploy-mode cluster --<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkP</span></span></span><br></pre></td></tr></table></figure>

<p>在集群模式下，Driver 运行在 Application Master 上，Application Master 进程同时负责驱动 Application 和从 YARN 中申请资源。该进程运行在 YARN Container 内，所以启动Application Master 的 Client 可以立即关闭，而不必持续到 Application 的声明周期。</p>
<p><img src="http://img.rogermaster.top/uPic/Pz8yvg.jpg" alt="Pz8yvg"></p>
<p>（1）Client(客户端)生成作业信息提交给 ResourceManager。</p>
<p>（2）ResourceManager 在某一个 NodeManager （由 YARN 决定）启动 Container，并将Application Master 分配给该 NodeManager。</p>
<p>（3）NodeManager 接收到 ResourceManager 的分配，启动 Application Master 并初始化作业，此时 NodeManager 就称为 Driver。</p>
<p>（4）Application 向 ResourceManager 申请资源，ResourceManager 分配资源的同时通知其他 NodeManager 启动相应的 Executor。</p>
<p>（5）Executfor 向 NodeManager 上的 Application Master 注册汇报并完成相应的任务。图 1-19 是 YARN 客户端模式的作业运行流程。Application Master 仅仅从 YARN 中申请资源给 Executor，之后 Client 会与 Container通信进行作业的调度。</p>
<p>应用的运行结果不能在客户端显示（可以在 history server 中查看），所以最好将结果保存在 HDFS 而非 stdout 输出，客户端的终端显示的是作为 Yarn 的 job 的简单运行状况。在此模式下,Driver运行在AM(ApplicationMaster)里，可以理解为AM包括了Driver的功能就像Driver运行在AM里一样，此时的AM既能够向AM申请资源并进行分配，又能完成Driver划分RDD提交task等工作。</p>
<h4 id="Yarn-client"><a href="#Yarn-client" class="headerlink" title="Yarn-client"></a>Yarn-client</h4><p>提交命令：</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  --master yarn --deploy-mode client --<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">Spark</span></span></span><br></pre></td></tr></table></figure>

<p>下图是Yarn-client模式的作业运行流程。Application Master 仅仅从 YARN 中申请资源给 Etecutia，之后 Client会与 Container 通信进行作业的调度。</p>
<p><img src="http://img.rogermaster.top/uPic/AjwTGM.jpg" alt="AjwTGM"></p>
<p>YARN-Client 模式的作业运行流程描述如下。</p>
<p>（1）客户端生成作业信息提交给 ResourceManager。</p>
<p>（2）ResourceManager 在本地 NodeManager启动 Container，并将 Application Master分配给该 NodeManager。</p>
<p>（3）NodeManager 接收到 ResourceManager 的分配，启动 Application Master 并初始化作业，此时这个 NodeManager 就称为 Driver。</p>
<p>（4）Application 向 ResourceManager 申请资源，ResourceManager 分配资源同时通知其他 NodeMamager 启动相应的 Executor。</p>
<p>（5）Executor 向本地启动的 Application Master 注册汇报并完成相应的任务。</p>
<p>Driver运行在客户端上，先有driver再用AM，此时driver负责RDD生成、task生成和分发，向AM申请资源等 ,AM负责向RM申请资源，其他的都由driver来完成。</p>
<p>从上面两张图可看出 YARN-Cluster 和 YARN-Client 的区别。在 YARN-Cluster模式下，SparkDriver 运行在 Application Master（AM）中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN上运行,所以 YARN-Cluster 模式不适合运行交互类型的作业。然而在 YARN-Client 模式下,AM 仅仅向 YARN 请求 Executor，Client 会与请求得到的 Container 通信来调度它们工作，也就是说 Client 不能离开。</p>
<p>总结起来就是，集群模式的 Spark Driver 运行在 AM 中，而客户端模式的 Spark Driven运行在客户端。所以，YARN-Cluster 适用于生产，而 YARN-Client 适用于交互和调试，也就是希望快速地看到应用的输出信息。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/Java大数据开发入门系列-三-————Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/Java大数据开发入门系列-三-————Hive/" class="post-title-link" itemprop="url">Java大数据开发入门系列(三)————Hive</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-08-18 22:41:06 / 修改时间：23:55:59" itemprop="dateCreated datePublished" datetime="2020-08-18T22:41:06+08:00">2020-08-18</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="文章更新中，请耐心期待"><a href="#文章更新中，请耐心期待" class="headerlink" title="文章更新中，请耐心期待"></a>文章更新中，请耐心期待</h1>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/Java大数据开发入门系列-二-————HDFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/Java大数据开发入门系列-二-————HDFS/" class="post-title-link" itemprop="url">Java大数据开发入门系列(二)————HDFS</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-08-18 22:40:21" itemprop="dateCreated datePublished" datetime="2020-08-18T22:40:21+08:00">2020-08-18</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-08-28 13:46:13" itemprop="dateModified" datetime="2020-08-28T13:46:13+08:00">2020-08-28</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><p>HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。</p>
<h1 id="二、起因"><a href="#二、起因" class="headerlink" title="二、起因"></a>二、起因</h1><p>要发展大数据，首要问题就是考虑数据存储的问题，大数据存储存在以下核心问题：</p>
<ul>
<li>数据存储容量的问题，既然大数据要解决的是数以PB计的数据计算问题，而一般的服务器磁盘容量通常1-2TB，那么如何存储这么大规模的数据。</li>
<li>数据读写速度的问题，一般磁盘的连续读写速度为几十MB，以这样的速度，几十PB的数据恐怕要读写到天荒地老。</li>
<li>数据可靠性的问题，磁盘大约是计算机设备中最易损坏的硬件了，在网站一块磁盘使用寿命大概是一年，如果磁盘损坏了，数据怎么办？</li>
</ul>
<p>在大数据技术出现之前，人们就需要面对这些关于存储的问题，对应的解决方案就是RAID技术。RAID（独立磁盘冗余阵列）技术主要是为了改善磁盘的存储容量，读写速度，增强磁盘的可用性和容错能力。目前服务器级别的计算机都支持插入多块磁盘（8块或者更多），通过使用RAID技术，实现数据在多块磁盘上的并发读写和数据备份。</p>
<p><img src="http://img.rogermaster.top/uPic/7GcG3f.jpg" alt="常用RAID技术原理"></p>
<h2 id="1-RAID-0"><a href="#1-RAID-0" class="headerlink" title="1.RAID 0"></a>1.RAID 0</h2><p>数据在从内存缓冲区写入磁盘时，根据磁盘数量将数据分成<em>N</em>份，这些数据同时并发写入<em>N</em>块磁盘，使得数据整体写入速度是一块磁盘的<em>N</em>倍。读取的时候也一样，因此RAID0具有极快的数据读写速度，但是RAID0不做数据备份，<em>N</em>块磁盘中只要有一块损坏，数据完整性就被破坏，所有磁盘的数据都会损坏。</p>
<h2 id="2-RAID-1"><a href="#2-RAID-1" class="headerlink" title="2.RAID 1"></a>2.RAID 1</h2><p>数据在写入磁盘时，将一份数据同时写入两块磁盘，这样任何一块磁盘损坏都不会导致数据丢失，插入一块新磁盘就可以通过复制数据的方式自动修复，具有极高的可靠性。</p>
<h2 id="3-RAID-10"><a href="#3-RAID-10" class="headerlink" title="3.RAID 10"></a>3.RAID 10</h2><p>结合RAID0和RAID1两种方案，将所有磁盘平均分成两份，数据同时在两份磁盘写入，相当于RAID1，但是在每一份磁盘里面的<em>N</em>/2块磁盘上，利用RAID0技术并发读写，既提高可靠性又改善性能，不过RAID10的磁盘利用率较低，有一半的磁盘用来写备份数据。</p>
<h2 id="4-RAID5"><a href="#4-RAID5" class="headerlink" title="4.RAID5"></a>4.<strong>RAID5</strong></h2><p>相比RAID3，更多被使用的方案是RAID5。RAID5和RAID3很相似，但是校验数据不是写入第<em>N</em>块磁盘，而是螺旋式地写入所有磁盘中。这样校验数据的修改也被平均到所有磁盘上，避免RAID3频繁写坏一块磁盘的情况。</p>
<h2 id="5-RAID6"><a href="#5-RAID6" class="headerlink" title="5.RAID6"></a>5.<strong>RAID6</strong></h2><p>如果数据需要很高的可靠性，在出现同时损坏两块磁盘的情况下（或者运维管理水平比较落后，坏了一块磁盘但是迟迟没有更换，导致又坏了一块磁盘），仍然需要修复数据，这时候可以使用RAID6。</p>
<p>RAID6和RAID5类似，但是数据只写入<em>N</em>-2块磁盘，并螺旋式地在两块磁盘中写入校验信息（使用不同算法生成）。</p>
<table>
<thead>
<tr>
<th><strong>RAID 等级</strong></th>
<th><strong>RAID0</strong></th>
<th><strong>RAID1</strong></th>
<th><strong>RAID3</strong></th>
<th><strong>RAID5</strong></th>
<th><strong>RAID6</strong></th>
<th><strong>RAID10</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>别名</strong></td>
<td>条带</td>
<td>镜像</td>
<td>专用奇偶校验条带</td>
<td>分布奇偶校验条带</td>
<td>双重奇偶校验条带</td>
<td>镜像加条带</td>
</tr>
<tr>
<td><strong>容错性</strong></td>
<td>无</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td><strong>冗余类型</strong></td>
<td>无</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td><strong>热备份选择</strong></td>
<td>无</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td><strong>读性能</strong></td>
<td>高</td>
<td>低</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td><strong>随机写性能</strong></td>
<td>高</td>
<td>低</td>
<td>低</td>
<td>一般</td>
<td>低</td>
<td>一般</td>
</tr>
<tr>
<td><strong>连续写性能</strong></td>
<td>高</td>
<td>低</td>
<td>低</td>
<td>低</td>
<td>低</td>
<td>一般</td>
</tr>
<tr>
<td><strong>需要磁盘数</strong></td>
<td>n≥1</td>
<td>2n (n≥1)</td>
<td>n≥3</td>
<td>n≥3</td>
<td>n≥4</td>
<td>2n(n≥2)≥4</td>
</tr>
<tr>
<td><strong>可用容量</strong></td>
<td>全部</td>
<td>50%</td>
<td>(n-1)/n</td>
<td>(n-1)/n</td>
<td>(n-2)/n</td>
<td>50%</td>
</tr>
</tbody></table>
<p>RAID技术只是在单台服务器的多块磁盘上组成阵列，大数据需要更大规模的存储空间和访问速度。因此，Hadoop结合前面的RAID技术和Google提出的GFS论文，创造了HDFS。HDFS（Hadoop分布式文件系统）是根据GFS（Google文件系统）的原理开发的，是GFS的简化版。</p>
<h1 id="三、HDFS架构原理"><a href="#三、HDFS架构原理" class="headerlink" title="三、HDFS架构原理"></a>三、HDFS架构原理</h1><p><img src="http://img.rogermaster.top/uPic/2vdRRD.jpg" alt="HDFS的架构"></p>
<p>HDFS的架构：主从架构，三大角色</p>
<ol>
<li>Namenode负责整个分布式文件系统的元数据（MetaData）管理，也就是文件路径名，数据block的ID以及存储位置等信息，承担着操作系统中文件分配表（FAT）的角色。HDFS为了保证数据的高可用，会将一个block复制为多份（缺省情况为3份），并将三份相同的block存储在不同的服务器上。这样当有磁盘损坏或者某个DataNode服务器宕机导致其存储的block不能访问的时候，Client会查找其备份的block进行访问。</li>
<li>Datanode负责文件数据的存储和读写操作，HDFS将文件数据分割成若干块（block），每个DataNode存储一部分block，这样文件就分布存储在整个HDFS服务器集群中。</li>
<li>SecondaryNamenode严格意义上来说并不属于namenode的备份节点，它主要起到的作用其实是替namenode分担压力，降低负载（元数据的编辑日志合并，也就是edits log）之用</li>
</ol>
<h2 id="1-心跳机制"><a href="#1-心跳机制" class="headerlink" title="1.心跳机制"></a>1.心跳机制</h2><p>为了保证集群的高可用性和高可靠性(HA)，DataNode会通过心跳和NameNode保持通信，如果DataNode超时未发送心跳，NameNode就会认为这个DataNode已经失效，立即查找这个DataNode上存储的block有哪些，以及这些block还存储在哪些服务器上，随后通知这些服务器再复制一份block到其他服务器上，保证HDFS存储的block备份数符合用户设置的数目，即使再有服务器宕机，也不会丢失数据。</p>
<p><img src="http://img.rogermaster.top/uPic/kadksT.jpg" alt="心跳机制"></p>
<p>1.NameNode启动之后，会开一个ipc server。NameNode 全权管理数据块的复制，它周期性从集群中的每个 DataNode 接收心跳信号和 block 状态报告，接收到心跳信号意味着该 DataNode 节点工作正常，块状态报告包含了该 DataNode 上所有数据块的列表</p>
<p>2.DataNode启动，连接NameNode注册，每隔3s向NameNode发送一个心跳，并携带状态信息，周期性地向 NameNode 上报 block 报告。NameNode 返回对该 DataNode 的指令，如将数据块复制到另一台机器，或删除某个数据块等，而当某一个 DataNode 超过10min还没向 NameNode 发送心跳，此时 NameNode 就会判定该 DataNode 不可用，此时客户端的读写操作就不会再传达到该 DataNode 上。</p>
<h2 id="2-安全模式"><a href="#2-安全模式" class="headerlink" title="2.安全模式"></a>2.安全模式</h2><p>Hadoop 集群刚开始启动时会进入安全模式，就用到了心跳机制。在集群刚启动的时候，每一个 DataNode 都会向 NameNode 发送 block 报告，NameNode 会统计它们上报的总block数，除以一开始知道的总个数total，当 block/total &lt; 99.99% 时，会触发安全模式，安全模式下客户端就没法向HDFS写数据，只能进行读数据。</p>
<p>Namenode感知Datanode掉线死亡时间的计算公式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval</span><br></pre></td></tr></table></figure>

<p>HDFS默认超时时间为630秒，heartbeat.recheck.interval(重新检查的时间间隔) 的默认值为5分钟，而 dfs.heartbeat.interval(发送一次心跳的间隔) 默认值为3秒。</p>
<p>安全模式不仅仅是集群刚启动时等所有的Datanode汇报这一种情况会进入安全模式的，还有就是HDFS数据块丢失达到一个比例的时候，也会自动进入，这个比例默认是0.1%，1000个块丢1个已经很严重的事件了。</p>
<h1 id="四、HDFS的使用"><a href="#四、HDFS的使用" class="headerlink" title="四、HDFS的使用"></a>四、HDFS的使用</h1><p>关于java api的说明：</p>
<p>FileSystem 是所有 HDFS 操作的主入口。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 这里我启动的是单节点的 Hadoop,所以副本系数设置为 1,默认值为 3</span></span><br><span class="line">configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"1"</span>);</span><br><span class="line"><span class="comment">// HDFS_PATH hdfs连接地址，HDFS_USER hdfs连接用户</span></span><br><span class="line">FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_PATH), configuration, HDFS_USER);</span><br></pre></td></tr></table></figure>

<p><strong>1. 显示当前目录结构</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  -R  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示根目录下内容</span></span><br><span class="line">hadoop fs -ls  /</span><br></pre></td></tr></table></figure>

<p>java API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileStatus[] statuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br></pre></td></tr></table></figure>

<p><strong>2. 创建目录</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">hadoop fs -mkdir  &lt;path&gt; </span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归创建目录</span></span><br><span class="line">hadoop fs -mkdir -p  &lt;path&gt;</span><br></pre></td></tr></table></figure>

<p>java API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/test0/"</span>));</span><br></pre></td></tr></table></figure>

<p><strong>3. 删除操作</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除文件</span></span><br><span class="line">hadoop fs -rm  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归删除目录和文件</span></span><br><span class="line">hadoop fs -rm -R  &lt;path&gt;</span><br></pre></td></tr></table></figure>

<p>java API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//返会 Boolean类型</span></span><br><span class="line">fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/b.txt"</span>), <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

<p><strong>4. 从本地加载文件到 HDFS</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -put  [localsrc] [dst] </span><br><span class="line">hadoop fs - copyFromLocal [localsrc] [dst]</span><br></pre></td></tr></table></figure>

<p>java API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下</span></span><br><span class="line">Path src = <span class="keyword">new</span> Path(<span class="string">"D:\\BigData-Notes\\notes\\installation"</span>);</span><br><span class="line">Path dst = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/"</span>);</span><br><span class="line">fileSystem.copyFromLocalFile(src, dst);</span><br></pre></td></tr></table></figure>

<p><strong>5. 从 HDFS 导出文件到本地</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -get  [dst] [localsrc] </span><br><span class="line">hadoop fs -copyToLocal [dst] [localsrc]</span><br></pre></td></tr></table></figure>

<p>java API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Path src = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/kafka.tgz"</span>);</span><br><span class="line">Path dst = <span class="keyword">new</span> Path(<span class="string">"D:\\app\\"</span>);</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 第一个参数控制下载完成后是否删除源文件,默认是 true,即删除;</span></span><br><span class="line"><span class="comment"> * 最后一个参数表示是否将 RawLocalFileSystem 用作本地文件系统;</span></span><br><span class="line"><span class="comment"> * RawLocalFileSystem 默认为 false,通常情况下可以不设置,</span></span><br><span class="line"><span class="comment"> * 但如果你在执行时候抛出 NullPointerException 异常,则代表你的文件系统与程序可能存在不兼容的情况 (window 下常见),</span></span><br><span class="line"><span class="comment"> * 此时可以将 RawLocalFileSystem 设置为 true</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">fileSystem.copyToLocalFile(<span class="keyword">false</span>, src, dst, <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

<p><strong>6. 查看文件内容</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -text  &lt;path&gt; </span><br><span class="line">hadoop fs -cat  &lt;path&gt;</span><br></pre></td></tr></table></figure>

<p>java API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>));</span><br><span class="line">String context = inputStreamToString(inputStream, <span class="string">"utf-8"</span>);</span><br><span class="line">System.out.println(context);</span><br></pre></td></tr></table></figure>

<p><strong>7. 显示文件的最后一千字节</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -tail  &lt;path&gt; </span><br><span class="line"><span class="meta">#</span><span class="bash"> 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节</span></span><br><span class="line">hadoop fs -tail -f  &lt;path&gt;</span><br></pre></td></tr></table></figure>

<p><strong>8. 拷贝文件</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp [src] [dst]</span><br></pre></td></tr></table></figure>

<p><strong>9. 移动文件</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv [src] [dst]</span><br></pre></td></tr></table></figure>

<p><strong>10. 统计当前目录下各文件大小</strong></p>
<p>shell命令：</p>
<ul>
<li>默认单位字节</li>
<li>-s : 显示所有文件大小总和，</li>
<li>-h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -du  &lt;path&gt;</span><br></pre></td></tr></table></figure>

<p><strong>11. 合并下载多个文件</strong></p>
<p>shell命令：</p>
<ul>
<li>-nl 在每个文件的末尾添加换行符（LF）</li>
<li>-skip-empty-file 跳过空文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -getmerge</span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml</span></span><br><span class="line">hadoop fs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml</span><br></pre></td></tr></table></figure>

<p><strong>12. 统计文件系统的可用空间信息</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -df -h /</span><br></pre></td></tr></table></figure>

<p><strong>13. 更改文件复制因子</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子</li>
<li>-w : 请求命令是否等待复制完成</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 示例</span><br><span class="line">hadoop fs -setrep -w 3 /user/hadoop/dir1</span><br></pre></td></tr></table></figure>

<p><strong>14. 权限控制</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 权限控制和Linux上使用方式一致</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chgrp [-R] GROUP URI [URI ...]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件的拥有者  用户必须是超级用户。</span></span><br><span class="line">hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</span><br></pre></td></tr></table></figure>

<p><strong>15. 文件检测</strong></p>
<p>shell命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -test - [defsz]  URI</span><br></pre></td></tr></table></figure>

<p>可选选项：</p>
<ul>
<li>-d：如果路径是目录，返回 0。</li>
<li>-e：如果路径存在，则返回 0。</li>
<li>-f：如果路径是文件，则返回 0。</li>
<li>-s：如果路径不为空，则返回 0。</li>
<li>-r：如果路径存在且授予读权限，则返回 0。</li>
<li>-w：如果路径存在且授予写入权限，则返回 0。</li>
<li>-z：如果文件长度为零，则返回 0。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 示例</span></span><br><span class="line">hadoop fs -test -e filename</span><br></pre></td></tr></table></figure>

<p>java API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 返会Boolean类型</span></span><br><span class="line">fileSystem.exists(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>));</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/Java大数据开发入门系列-一-————环境搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/Java大数据开发入门系列-一-————环境搭建/" class="post-title-link" itemprop="url">Java大数据开发入门系列(一)————环境搭建</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-08-18 22:39:25" itemprop="dateCreated datePublished" datetime="2020-08-18T22:39:25+08:00">2020-08-18</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-08-31 00:29:53" itemprop="dateModified" datetime="2020-08-31T00:29:53+08:00">2020-08-31</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><ul>
<li><p>MacOS10.15</p>
</li>
<li><p>Parallels Desktop16创建搭建的centos8 虚拟机</p>
</li>
<li><p>集群环境：</p>
<table>
<thead>
<tr>
<th>IP地址</th>
<th>主机名</th>
<th>操作系统</th>
<th>角色</th>
<th>服务</th>
</tr>
</thead>
<tbody><tr>
<td>10.211.55.8/24</td>
<td>master</td>
<td>Centos8</td>
<td>master</td>
<td>HDFS(NameNode)、DFSZKFailoverController(zkfc)、SYNC(同步文件服务器)、ResourceManager(资源分配与调度)</td>
</tr>
<tr>
<td>10.211.55.9/24</td>
<td>slave1</td>
<td>Centos8</td>
<td>slave1</td>
<td>Zookeeper、HDFS(SecondaryNamenode)、MapReduce(JobHistoryServer)、NodeManager</td>
</tr>
<tr>
<td>10.211.55.10/24</td>
<td>slave2</td>
<td>Centos8</td>
<td>Slave2</td>
<td>Zookeeper、HDFS(DataNode)、NodeManager</td>
</tr>
<tr>
<td>10.211.55.11/24</td>
<td>slave3</td>
<td>Centos8</td>
<td>Slave3</td>
<td>Zookeeper、HDFS(DataNode)、NodeManager</td>
</tr>
</tbody></table>
</li>
</ul>
<h1 id="开始搭建"><a href="#开始搭建" class="headerlink" title="开始搭建"></a>开始搭建</h1><h2 id="防火墙和SELINUX设置"><a href="#防火墙和SELINUX设置" class="headerlink" title="防火墙和SELINUX设置"></a>防火墙和SELINUX设置</h2><p>因为Hadoop需要开启的端口很多，而且牵涉到很多的权限，所以我们在测试时将防火墙和SELINUX都关掉。<br>在生产环境中，需要针对不同的开放端口做针对性的设置。</p>
<h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><p>运行以下命令，关闭防火墙</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 临时关闭防火墙 和 禁止开机启动防火墙</span></span><br><span class="line">systemctl stop firewalld &amp;&amp; systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">systemctl status  firewalld	   <span class="comment">#查看防火墙状态。</span></span><br></pre></td></tr></table></figure>

<h3 id="关闭SELINUX"><a href="#关闭SELINUX" class="headerlink" title="关闭SELINUX"></a>关闭SELINUX</h3><p>运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config         <span class="comment">#SELINUX配置文件</span></span><br></pre></td></tr></table></figure>

<p>相关参数修改如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#SELINUX=enforcing</span></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure>

<p>重启服务器，然后查看SELINUX状态：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reboot       <span class="comment">#重启</span></span><br><span class="line">getenforce   <span class="comment">#查询SELinux的运行模式,permissive（宽容模式）；enforcing（强制模式）；</span></span><br><span class="line">/usr/sbin/sestatus -v  <span class="comment">#查看SELINUX的状态</span></span><br></pre></td></tr></table></figure>

<p>显示如下内容，则说明SELINUX已经关闭了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELinux status:                 disabled</span><br></pre></td></tr></table></figure>

<h2 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h2><p>以master(10.211.55.8)节点为例，运行下面的命令，修改本机的hostname</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">修改hostname</span></span><br><span class="line">hostnamectl set-hostname master</span><br></pre></td></tr></table></figure>

<p>运行下面的命令查看设置好的hostname</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/hostname</span><br></pre></td></tr></table></figure>

<p>如果显示如下的内容，则说明修改成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">master</span><br></pre></td></tr></table></figure>

<p>其他节点(10.211.55.9~11)同样进行以上操作，修改主机名。</p>
<h2 id="hosts设置"><a href="#hosts设置" class="headerlink" title="hosts设置"></a>hosts设置</h2><p>由于一次次的远程连接需要输入IP地址，不利于管理和使用，我们可以在hosts里面把服务器的hostname跟IP地址对应起来。</p>
<p>在master(10.211.55.8)输入以下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure>

<p>设置成以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">10.211.55.8 master</span><br><span class="line">10.211.55.9 slave1</span><br><span class="line">10.211.55.10 slave2</span><br><span class="line">10.211.55.11 slave3</span><br></pre></td></tr></table></figure>

<p>其他节点(10.211.55.9~11)同样进行以上操作，修改hosts。</p>
<h2 id="添加Hadoop用户"><a href="#添加Hadoop用户" class="headerlink" title="添加Hadoop用户"></a>添加Hadoop用户</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd hadoop &amp;&amp; passwd hadoop</span><br></pre></td></tr></table></figure>

<p>命令输入完毕后设置用户密码就行</p>
<p>可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题，执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visudo</span><br></pre></td></tr></table></figure>

<p>Shell 命令</p>
<p>如下图，找到 <code>root ALL=(ALL) ALL</code> 这行（应该在第98行，可以先按一下键盘上的 <code>ESC</code> 键，然后输入 <code>:98</code> (按一下冒号，接着输入98，再按回车键)，可以直接跳到第98行 ），然后在这行下面增加一行内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop  ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure>

<p>如下图所示：</p>
<p><img src="http://img.rogermaster.top/uPic/RhXSSv.png" alt="RhXSSv"></p>
<p>切换到hadoop用户</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su hadoop</span><br><span class="line">cd ~</span><br></pre></td></tr></table></figure>

<p>每个节点都执行一次</p>
<h2 id="配置免密登录"><a href="#配置免密登录" class="headerlink" title="配置免密登录"></a>配置免密登录</h2><p>在所有的节点上执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa         <span class="comment">#生成验证密钥</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub|ssh hadoop@master <span class="string">"cat - &gt;&gt; ~/.ssh/authorized_keys"</span> <span class="comment">#发送给主服务器</span></span><br></pre></td></tr></table></figure>

<p>如果需要互相免密码登录，则master执行下面命令，把密钥分发给从服务器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/authorized_keys</span><br><span class="line">scp ~/.ssh/authorized_keys hadoop@slave2:~/.ssh/authorized_keys</span><br><span class="line">scp ~/.ssh/authorized_keys hadoop@slave3:~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>在所有的节点执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>退出hadoop用户，再重新 su hadoop进入一次即可免密登录了</p>
<p>验证免密登录</p>
<p><img src="http://img.rogermaster.top/uPic/oE23jf.png" alt="oE23jf"></p>
<h2 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h2><p>创建upload目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir upload</span><br></pre></td></tr></table></figure>

<p>将下载下来的jdk上传到master节点的<code>/home/hadoop/upload</code>下，<a href="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html" title="jdk下载" target="_blank" rel="noopener">jdk下载地址</a></p>
<p>执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /usr/java</span><br><span class="line"><span class="built_in">cd</span> /home/hadoop/upload</span><br><span class="line">tar -zxvf jdk-8u261-linux-x64.tar.gz    <span class="comment">#解压JDK</span></span><br><span class="line">sudo sudo mv jdk1.8.0_261/ /usr/java/             <span class="comment">#将JDK移动(剪切)到/usr/java/目录</span></span><br></pre></td></tr></table></figure>

<p>修改全局环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/profile       <span class="comment">#文件底部添加以下内容</span></span><br></pre></td></tr></table></figure>

<p>在文件底部添加以下内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jdk</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/usr/java/jdk1.8.0_261"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>

<p>加载新的全局环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>执行以下命令验证jdk是否安装成功</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<p><img src="http://img.rogermaster.top/uPic/LZmlJj.png" alt="LZmlJj"></p>
<p>将jdk发送到其他节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/java</span><br><span class="line"></span><br><span class="line">ssh root@slave1 'mkdir -p /usr/java/jdk1.8.0_261' &amp;&amp; sudo scp -r /usr/java/jdk1.8.0_261/ root@slave1:$PWD</span><br><span class="line"></span><br><span class="line">ssh root@slave2 'mkdir -p /usr/java/jdk1.8.0_261' &amp;&amp; sudo scp -r /usr/java/jdk1.8.0_261/ root@slave2:$PWD</span><br><span class="line"></span><br><span class="line">ssh root@slave3 'mkdir -p /usr/java/jdk1.8.0_261' &amp;&amp; sudo scp -r /usr/java/jdk1.8.0_261/ root@slave3:$PWD</span><br></pre></td></tr></table></figure>

<p>将环境变量发送到其他节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo scp /etc/profile root@slave1:/etc/profile</span><br><span class="line"></span><br><span class="line">sudo scp /etc/profile root@slave2:/etc/profile</span><br><span class="line"></span><br><span class="line">sudo scp /etc/profile root@slave3:/etc/profile</span><br></pre></td></tr></table></figure>

<p>在slave1、slave2、slave3上分别执行以下命令，让环境变量生效</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<h2 id="安装Zookeeper"><a href="#安装Zookeeper" class="headerlink" title="安装Zookeeper"></a>安装Zookeeper</h2><h2 id="安装并配置Zookeeper"><a href="#安装并配置Zookeeper" class="headerlink" title="安装并配置Zookeeper"></a>安装并配置Zookeeper</h2><p>由于Zookeeper类似于民主选举，每台服务器分别投票共同选举一个作为leader，剩下的都是follower。基于这个原因，官方建议服务器集群设置为奇数台，偶数台的话会有一台的资源浪费。根据咱们的集群规划：slave1~3为我们的Zookeeper服务器。</p>
<p>在slave1执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/hadoop/upload</span><br><span class="line"></span><br><span class="line">su root</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /home/hadoop/upload</span><br><span class="line"></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz</span><br><span class="line"></span><br><span class="line">mkdir /home/hadoop/server &amp;&amp; mkdir -p /home/hadoop/data/zkdata &amp;&amp; mkdir -p /home/hadoop/<span class="built_in">log</span>/zklog</span><br><span class="line"></span><br><span class="line">mv apache-zookeeper-3.6.1-bin/ /home/hadoop/server/</span><br><span class="line"></span><br><span class="line">sudo chown -R hadoop:hadoop /home/hadoop/*</span><br></pre></td></tr></table></figure>

<p>修改全局环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/profile       <span class="comment">#文件底部添加以下内容</span></span><br></pre></td></tr></table></figure>

<p>在文件底部添加以下内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># zookeeper</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=<span class="string">"/home/hadoop/zookeeper"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$ZOOKEEPER_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>

<p>加载新的全局环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile  </span><br><span class="line"><span class="comment"># 切换到hadoop用户</span></span><br><span class="line">su hadoop</span><br></pre></td></tr></table></figure>

<p>将Zookeeper发送到slave2、slave3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop</span><br><span class="line"></span><br><span class="line">ssh root@slave2 'mkdir -p /home/hadoop/server &amp;&amp; mkdir -p /home/hadoop/data/zkdata &amp;&amp; mkdir -p /home/hadoop/log/zklog' &amp;&amp; sudo scp -r /home/hadoop/server/ root@slave2:$PWD &amp;&amp; sudo scp -r /home/hadoop/data/ root@slave2:$PWD &amp;&amp; sudo scp -r /home/hadoop/log/ root@slave2:$PWD</span><br><span class="line"></span><br><span class="line">ssh root@slave3 'mkdir -p /home/hadoop/server &amp;&amp; mkdir -p /home/hadoop/data/zkdata &amp;&amp; mkdir -p /home/hadoop/log/zklog' &amp;&amp; sudo scp -r /home/hadoop/server/ root@slave3:$PWD &amp;&amp; sudo scp -r /home/hadoop/data/ root@slave3:$PWD &amp;&amp; sudo scp -r /home/hadoop/log/ root@slave3:$PWD</span><br></pre></td></tr></table></figure>

<p>将环境变量发送到slave2、slave3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo scp /etc/profile root@slave2:/etc/profile &amp;&amp; sudo scp /etc/profile root@slave3:/etc/profile</span><br></pre></td></tr></table></figure>

<p>在slave2、slave3上分别执行以下命令，让环境变量生效</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>使用hadoop执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/hadoop/server/apache-zookeeper-3.6.1-bin/conf</span><br><span class="line">sudo cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<p>修改zoo.cfg的文件内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi zoo.cfg</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/home/hadoop/data/zkdata</span><br><span class="line">dataLogDir=/home/hadoop/log/zklog/</span><br><span class="line">server.1=slave1:2888:3888</span><br><span class="line">server.2=slave2:2888:3888</span><br><span class="line">server.3=slave3:2888:3888</span><br></pre></td></tr></table></figure>

<p>将zoo.cfg分发到slave2~3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo scp /home/hadoop/server/apache-zookeeper-3.6.1-bin/conf/zoo.cfg hadoop@slave2:/home/hadoop/server/apache-zookeeper-3.6.1-bin/conf/zoo.cfg &amp;&amp; sudo scp /home/hadoop/server/apache-zookeeper-3.6.1-bin/conf/zoo.cfg hadoop@slave3:/home/hadoop/server/apache-zookeeper-3.6.1-bin/conf/zoo.cfg</span><br></pre></td></tr></table></figure>

<p>最后不要忘了在每个服务器“/home/hadoop/data/zkdata/”下新建文件“myid”并把当前服务器编号写进去，举例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -R hadoop:hadoop /home/hadoop/*</span><br><span class="line"><span class="comment"># slave1是1，slave2是2，slave3是3</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /home/hadoop/data/zkdata/myid</span><br></pre></td></tr></table></figure>

<p>以下内容只能在slave1~3上执行才能看到正确的结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh stop    <span class="comment">#停止Zookeeper服务</span></span><br><span class="line">zkServer.sh start   <span class="comment">#开启Zookeeper服务</span></span><br><span class="line">zkServer.sh status   <span class="comment">#开启Zookeeper服务</span></span><br></pre></td></tr></table></figure>

<p>执行<code>zkServer.sh start</code>正常情况下会看到下面的内容：</p>
<p><img src="http://img.rogermaster.top/uPic/PW9TW7.png" alt="PW9TW7"></p>
<p>如果启动失败，可以到“/home/hadoop/zookeeper/logs/”这个目录里面看看启动日志。</p>
<p>录入下面的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>可以看到如下的结果：</p>
<p><img src="http://img.rogermaster.top/uPic/FHDioK.png" alt="FHDioK"></p>
<p>注意：虽然我们在配置文件中写明了服务器的列表信息，但是，我们还是需要去每一台服务 器去启动，不是一键启动集群模式。<br>每启动一台查看一下状态再启动下一台<br>三台机器上都要有QuorumPeerMain进程，都能显示follower或者leader</p>
<h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><p>把下载好的Hadoop安装包上传到master节点的/home/hadoop/upload目录下面。</p>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz" title="hadoop下载" target="_blank" rel="noopener">hadoop下载地址</a></p>
<p>先通过root账户执行以下操作</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">su root</span><br><span class="line">mkdir -p <span class="built_in">cd</span> /home/hadoop/server</span><br><span class="line"><span class="built_in">cd</span> /home/hadoop/server</span><br><span class="line">tar -zxvf hadoop-3.3.0.tar.gz</span><br><span class="line">mv /home/hadoop/upload/hadoop-3.3.0/ /home/hadoop/server/</span><br><span class="line">chown -R hadoop:hadoop /home/hadoop/*</span><br></pre></td></tr></table></figure>

<p>修改全局环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/profile       <span class="comment">#文件底部添加以下内容</span></span><br></pre></td></tr></table></figure>

<p>在文件底部添加以下内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hadoop</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="string">"/home/hadoop/server/hadoop-3.3.0"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>

<p>加载新的全局环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>输入<code>echo $HADOOP_HOME</code>显示如下则表明成功</p>
<p><img src="http://img.rogermaster.top/uPic/6RkiD2.png" alt="6RkiD2"></p>
<p>在其他节点同样执行以上添加环境变量的操作</p>
<p>进入master节点，创建相关目录，执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">su hadoop</span><br><span class="line">rm -rf /home/hadoop/server/hadoop-3.3.0/share/doc      <span class="comment">#删除文档，很大，又没用</span></span><br><span class="line">mkdir -p /home/hadoop/data/dfs/data &amp;&amp; mkdir /home/hadoop/data/dfs/name &amp;&amp; mkdir /home/hadoop/data/dfs/tmp &amp;&amp; mkdir /home/hadoop/data/journaldata</span><br><span class="line"><span class="built_in">cd</span> /home/hadoop/server/hadoop-3.3.0/etc/hadoop</span><br></pre></td></tr></table></figure>

<h3 id="修改hadoop-env"><a href="#修改hadoop-env" class="headerlink" title="修改hadoop-env"></a>修改hadoop-env</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/server/hadoop-3.3.0/etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure>

<p>添加一行内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/usr/java/jdk1.8.0_261"</span></span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件core-site-xml"><a href="#修改配置文件core-site-xml" class="headerlink" title="修改配置文件core-site.xml"></a>修改配置文件core-site.xml</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/hadoop/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定hdfs的nameservice为master --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:<span class="comment">//master:9000&lt;/value&gt;</span></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定hadoop临时目录 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/data/dfs/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定zookeeper地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;slave1:<span class="number">2181</span>,slave2:<span class="number">2181</span>,slave3:<span class="number">2181</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- hadoop链接zookeeper的超时时长设置 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="number">1000</span>&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;ms&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 修改core-site.xml中的ipc参数,防止出现连接journalnode服务ConnectException --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;<span class="number">100</span>&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;Indicates the number of retries a client will make to establish a server connection.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;<span class="number">10000</span>&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;Indicates the number of milliseconds a client will wait <span class="keyword">for</span> before retrying to establish a server connection.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;topology.script.file.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;<span class="number">10000</span>&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;Indicates the number of milliseconds a client will wait <span class="keyword">for</span> before retrying to establish a server connection.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件hdfs-site-xml"><a href="#修改配置文件hdfs-site-xml" class="headerlink" title="修改配置文件hdfs-site.xml"></a>修改配置文件hdfs-site.xml</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定副本数 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 配置namenode和datanode的工作目录-数据存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 启用webhdfs --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 配置HDFS的权限控制 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">	    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 配置SecondaryNameNode的节点地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件mapred-site-xml"><a href="#修改配置文件mapred-site-xml" class="headerlink" title="修改配置文件mapred-site.xml"></a>修改配置文件mapred-site.xml</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/hadoop/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>全部内容如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定mr框架为yarn方式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     </span><br><span class="line">    <span class="comment">&lt;!-- 指定mapreduce jobhistory地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     </span><br><span class="line">    <span class="comment">&lt;!-- 任务历史服务器的web地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件yarn-site-xml"><a href="#修改配置文件yarn-site-xml" class="headerlink" title="修改配置文件yarn-site.xml"></a>修改配置文件yarn-site.xml</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/hadoop/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>内容如下</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 以逗号分隔的服务列表，其中服务名称应仅包含a-zA-Z0-9_并且不能以数字开头--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 配置ResourceManager的服务节点 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件workers"><a href="#修改配置文件workers" class="headerlink" title="修改配置文件workers"></a>修改配置文件workers</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/hadoop/etc/hadoop/workers</span><br></pre></td></tr></table></figure>

<p>文件中配置的是DataNode的所在节点服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave2</span><br><span class="line">slave3</span><br></pre></td></tr></table></figure>

<h3 id="修改yarn-env"><a href="#修改yarn-env" class="headerlink" title="修改yarn-env"></a>修改yarn-env</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/hadoop/etc/hadoop/yarn-env.sh</span><br></pre></td></tr></table></figure>

<p>添加一行内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/usr/java/jdk1.8.0_261"</span></span><br></pre></td></tr></table></figure>

<h3 id="修改mapred-env"><a href="#修改mapred-env" class="headerlink" title="修改mapred-env"></a>修改mapred-env</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /home/hadoop/hadoop/etc/hadoop/mapred-env.sh</span><br></pre></td></tr></table></figure>

<p>添加一行内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/usr/java/jdk1.8.0_261"</span></span><br></pre></td></tr></table></figure>

<h3 id="将hadoop安装包分发到其他集群节点"><a href="#将hadoop安装包分发到其他集群节点" class="headerlink" title="将hadoop安装包分发到其他集群节点"></a>将hadoop安装包分发到其他集群节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/hadoop/server</span><br><span class="line">scp -r /home/hadoop/server/hadoop-3.3.0/ hadoop@slave1:<span class="variable">$PWD</span> &amp;&amp; scp -r /home/hadoop/server/hadoop-3.3.0/ hadoop@slave2:<span class="variable">$PWD</span> &amp;&amp; scp -r /home/hadoop/server/hadoop-3.3.0/ hadoop@slave3:<span class="variable">$PWD</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /home/hadoop/data</span><br><span class="line">scp -r /home/hadoop/data/dfs/ hadoop@slave1:<span class="variable">$PWD</span> &amp;&amp; scp -r /home/hadoop/data/journaldata hadoop@slave1:<span class="variable">$PWD</span> &amp;&amp; scp -r /home/hadoop/data/dfs/ hadoop@slave2:<span class="variable">$PWD</span> &amp;&amp; scp -r /home/hadoop/data/journaldata hadoop@slave2:<span class="variable">$PWD</span> &amp;&amp; scp -r /home/hadoop/data/dfs/ hadoop@slave3:<span class="variable">$PWD</span> &amp;&amp; scp -r /home/hadoop/data/journaldata hadoop@slave3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>

<h2 id="格式化HDFS"><a href="#格式化HDFS" class="headerlink" title="格式化HDFS"></a>格式化HDFS</h2><p>在master节点中格式化HDFS(只在master节点，即NameNode执行)，执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h2 id="格式化zkfc"><a href="#格式化zkfc" class="headerlink" title="格式化zkfc"></a>格式化zkfc</h2><p>在master节点中格式化HDFS(只在master节点，即NameNode执行)，执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure>

<h2 id="启动Hadoop集群"><a href="#启动Hadoop集群" class="headerlink" title="启动Hadoop集群"></a>启动Hadoop集群</h2><h3 id="日常启动hadoop"><a href="#日常启动hadoop" class="headerlink" title="日常启动hadoop"></a>日常启动hadoop</h3><p>Hadoop启动顺序：Zookeeper-&gt;Hadoop-&gt;Hbase&gt;Hive…</p>
<h4 id="停止所有服务"><a href="#停止所有服务" class="headerlink" title="停止所有服务"></a>停止所有服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stop-all.sh                            <span class="comment">#最好在master（NameNode）上执行</span></span><br><span class="line">mapred --daemon stop historyserver 	   <span class="comment">#master上执行</span></span><br><span class="line">zkServer.sh stop                       <span class="comment">#slave1~3上操作</span></span><br></pre></td></tr></table></figure>

<h4 id="启动ZooKeeper"><a href="#启动ZooKeeper" class="headerlink" title="启动ZooKeeper"></a>启动ZooKeeper</h4><p>在slave1~3上分别执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh stop</span><br><span class="line">zkServer.sh start</span><br><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>显示如下内容，则启动成功</p>
<p><img src="http://img.rogermaster.top/uPic/0VF4xe.png" alt="0VF4xe"></p>
<h4 id="启动HDFS"><a href="#启动HDFS" class="headerlink" title="启动HDFS"></a>启动HDFS</h4><p>其中一台机器执行就OK了，比如：master</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh    <span class="comment">#先停掉服务</span></span><br><span class="line">start-dfs.sh   <span class="comment">#如果出现错误，则在hadoop-env.sh中，再显示地重新声明一遍JAVA_HOME</span></span><br></pre></td></tr></table></figure>

<p>显示内容如下：</p>
<p><img src="http://img.rogermaster.top/uPic/fjyyxM.png" alt="fjyyxM"></p>
<p>执行命令查看：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>master上显示如下：</p>
<p><img src="http://img.rogermaster.top/uPic/2yw9A8.png" alt="2yw9A8"></p>
<p>Slaver1~3显示如下：</p>
<p><img src="http://img.rogermaster.top/uPic/FYF63v.png" alt="FYF63v"></p>
<p><img src="http://img.rogermaster.top/uPic/pRRr0o.png" alt="pRRr0o"></p>
<h4 id="启动YARN"><a href="#启动YARN" class="headerlink" title="启动YARN"></a>启动YARN</h4><p>在master进行启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop-yarn.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>显示内容如下：</p>
<p><img src="http://img.rogermaster.top/uPic/y1tDm5.png" alt="y1tDm5"></p>
<p>执行命令查看：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>显示如下：</p>
<p><img src="http://img.rogermaster.top/uPic/vwyzNP.png" alt="vwyzNP"></p>
<p>若备用节点的 resourcemanager 没有启动起来，则手动启动起来</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>

<h4 id="启动-mapreduce-任务历史服务器"><a href="#启动-mapreduce-任务历史服务器" class="headerlink" title="启动 mapreduce 任务历史服务器"></a>启动 mapreduce 任务历史服务器</h4><p>在slave1上执行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mapred --daemon stop historyserver</span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p>执行命令查看：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>显示如下：</p>
<p><img src="http://img.rogermaster.top/uPic/JfbHLC.png" alt="JfbHLC"></p>
<h2 id="WEB控制台"><a href="#WEB控制台" class="headerlink" title="WEB控制台"></a>WEB控制台</h2><h3 id="hdfs控制台"><a href="#hdfs控制台" class="headerlink" title="hdfs控制台"></a>hdfs控制台</h3><p><a href="http://10.211.55.8:9870/dfshealth.html#tab-overview" target="_blank" rel="noopener">http://10.211.55.8:9870/dfshealth.html#tab-overview</a></p>
<p>![image-20200830213217841](/Users/roger/Library/Application Support/typora-user-images/image-20200830213217841.png)</p>
<h3 id="yarn控制台"><a href="#yarn控制台" class="headerlink" title="yarn控制台"></a>yarn控制台</h3><p><a href="http://10.211.55.8:8088/cluster" target="_blank" rel="noopener">http://10.211.55.8:8088/cluster</a></p>
<p><img src="http://img.rogermaster.top/uPic/nq8zHG.png" alt="nq8zHG"></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/SpringBoot中使用flyway做好数据库版本控制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/SpringBoot中使用flyway做好数据库版本控制/" class="post-title-link" itemprop="url">SpringBoot中使用flyway做好数据库版本控制</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-08-16 12:53:51 / 修改时间：17:51:52" itemprop="dateCreated datePublished" datetime="2020-08-16T12:53:51+08:00">2020-08-16</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spring-boot/" itemprop="url" rel="index"><span itemprop="name">spring boot</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="为什么需要数据库版本控制"><a href="#为什么需要数据库版本控制" class="headerlink" title="为什么需要数据库版本控制"></a>为什么需要数据库版本控制</h2><p>​    在真实的项目开发中，我们一般有三套环境：开发、测试、生产。在开发阶段我们一般都是在开发环境中进行操作，项目开发肯定不止一个人，在开发的过程中，我们肯定会对数据库的库表进行一些修改操作。并且这些操作需要同步到这三套环境中。但是多人协同，人为操作难免会出现疏忽，有时候修改了开发环境忘记了去修改其他环境。很多情况下都需要对数据库的变化做跟踪，以便于我们回退到某个版本。因此我们需要有这样一个工具来帮助我们管理数据库版本，做好各个环境同步更新。</p>
<h2 id="为什么选择Flyway"><a href="#为什么选择Flyway" class="headerlink" title="为什么选择Flyway"></a>为什么选择Flyway</h2><p>​    现在比较常用的数据库版本管理工具有Flyway和Liquibase。Spring Boot提供了这两者的内建支持，可以很快应用到产品中。</p>
<p>​    使用Flyway的好处在于使用简单，直接书写我们比较熟悉的sql脚本即可，不需要进行额外的学习。Liquibase的优点在于它能跨平台、跨库，但是需要我们花时间去学习他的脚本编写规则。如果你的项目中没有跨数据库的需要，那么flyway完全够用了。</p>
<h2 id="Flyway的工作模式"><a href="#Flyway的工作模式" class="headerlink" title="Flyway的工作模式"></a>Flyway的工作模式</h2><p>​    flyway在项目运行的时候会判断你的数据库中是否存在<em>flyway_schema_history</em>表，如果没有就会创建。这个表主要用于记录数据库的状态，Flyway的版本控住主要也是依赖这张表的。</p>
<p>​    当<em>flyway_schema_history</em>这张表存在，。当检测到你有新的版本需要迁移的时候，Flyway会逐一对比<em>flyway_schema_history</em>表中的已存在的版本记录，如果有未应用的Migrations，Flyway会获取这些Migrations并按版本号次序迁移到数据库中。</p>
<p><img src="http://img.rogermaster.top/uPic/TkDIKR.png" alt="TkDIKR"></p>
<p>应用每个迁移时，<em>flyway_schema_history</em>表将相应更新：</p>
<p><img src="http://img.rogermaster.top/uPic/GC3nO8.png" alt="GC3nO8"></p>
<p>​    flyway在升级数据库的时候，会检查已经执行过的版本对应的脚本是否发生变化，包括脚本文件名，以及脚本内容。如果flyway检测到发生了变化，则抛出错误，并终止升级。</p>
<p>​    如果已经执行过的脚本没有发生变化，flyway会跳过这些脚本，依次执行后续版本的脚本，并在记录表中插入对应的升级记录。</p>
<p>​    所以，flyway总是幂等的，而且可以支持跨版本的升级。</p>
<p>​    Migrations就是我们用SQL编写的脚本。为了让我们编写的SQL脚本生效，还需要按照Flyway指定的方式进行命名。</p>
<p><img src="http://img.rogermaster.top/uPic/nuNib6.png" alt="nuNib6"></p>
<ul>
<li><strong>Prefix</strong>: <code>V</code>代表版本化，<code>U</code>代表撤销，<code>R</code>代表可重复迁移。</li>
<li><strong>Version</strong>: 带点或下划线的版本，可以随意分隔多个部分（不适合重复迁移）。</li>
<li><strong>Separator</strong>: <code>__</code> (两个下划线)</li>
<li><strong>Description</strong>: 下划线或空格分隔单词</li>
<li><strong>Suffix</strong>: <code>.sql</code></li>
</ul>
<h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><h3 id="1、添加依赖"><a href="#1、添加依赖" class="headerlink" title="1、添加依赖"></a>1、添加依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.flywaydb<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flyway-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="2、配置application-yml文件"><a href="#2、配置application-yml文件" class="headerlink" title="2、配置application.yml文件"></a>2、配置application.yml文件</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line"><span class="attr">  datasource:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="attr">jdbc:mysql://localhost:3306/test?useSSL=false&amp;allowMultiQueries=true</span></span><br><span class="line"><span class="attr">    username:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">    password:</span> <span class="number">123456</span></span><br><span class="line"><span class="attr">    driver-class-name:</span> <span class="string">com.mysql.cj.jdbc.Driver</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据库版本控制</span></span><br><span class="line"><span class="attr">  flyway:</span></span><br><span class="line">    <span class="comment"># 启用或禁用 flyway</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 字符编码</span></span><br><span class="line"><span class="attr">    encoding:</span> <span class="string">utf-8</span></span><br><span class="line">    <span class="comment"># 对执行迁移时基准版本的描述</span></span><br><span class="line"><span class="attr">    baseline-description:</span> <span class="string">test</span></span><br><span class="line">    <span class="comment"># 若连接的数据库非空库，是否初始化</span></span><br><span class="line">    <span class="comment"># 当迁移时发现目标schema非空，而且带有没有元数据的表时，是否自动执行基准迁移，默认false.</span></span><br><span class="line"><span class="attr">    baseline-on-migrate:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 指定 baseline 的版本号,缺省值为 1, 低于该版本号的 SQL 文件, migrate 的时候被忽略</span></span><br><span class="line">    <span class="comment"># 开始执行基准迁移时对现有的schema的版本打标签，默认值为1.</span></span><br><span class="line"><span class="attr">    baseline-version:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 是否开启校验</span></span><br><span class="line">    <span class="comment"># 迁移时是否校验，默认为 true</span></span><br><span class="line"><span class="attr">    validate-on-migrate:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 默认脚本加载路径：/db/migration</span></span><br><span class="line">    <span class="comment"># locations: ["classpath:/db/migration"]</span></span><br><span class="line">    <span class="comment"># flyway 的 clean 命令会删除指定 schema 下的所有 table，默认 false</span></span><br><span class="line"><span class="attr">    clean-disabled:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 发环境最好开启 outOfOrder, 生产环境关闭 outOfOrder</span></span><br><span class="line">    <span class="comment"># 是否允许无序的迁移，默认 false</span></span><br><span class="line"><span class="attr">    out-of-order:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 检查迁移脚本的位置是否存在，默认false</span></span><br><span class="line"><span class="attr">    check-location:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 当读取元数据表时是否忽略错误的迁移，默认false</span></span><br><span class="line"><span class="attr">    ignore-future-migrations:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 当初始化好连接时要执行的SQL</span></span><br><span class="line"><span class="attr">    init-sqls:</span> <span class="string">show</span> <span class="string">tables;</span></span><br><span class="line">    <span class="comment"># 迁移时使用的JDBC URL，如果没有指定的话，将使用配置的主数据源</span></span><br><span class="line">    <span class="comment"># url:</span></span><br><span class="line">    <span class="comment"># 迁移数据库的用户名</span></span><br><span class="line">    <span class="comment"># user:</span></span><br><span class="line">    <span class="comment"># 目标数据库的密码</span></span><br><span class="line">    <span class="comment"># password:</span></span><br><span class="line">    <span class="comment"># 设置每个placeholder的前缀，默认$&#123;</span></span><br><span class="line">    <span class="comment">#placeholder-prefix:</span></span><br><span class="line">    <span class="comment"># 是否要被替换，默认true</span></span><br><span class="line">    <span class="comment">#placeholder-replacement:</span></span><br><span class="line">    <span class="comment"># 设置每个placeholder的后缀，默认&#125;</span></span><br><span class="line">    <span class="comment">#placeholder-suffix:</span></span><br><span class="line">    <span class="comment"># 设置placeholder的value</span></span><br><span class="line">    <span class="comment">#placeholders.[placeholder name]</span></span><br><span class="line">    <span class="comment"># 设定需要flywary迁移的schema，大小写敏感，默认为连接默认的schema</span></span><br><span class="line">    <span class="comment">#schemas: flyway</span></span><br><span class="line">    <span class="comment"># 迁移文件的前缀，默认为V</span></span><br><span class="line">    <span class="comment">#sql-migration-prefix:</span></span><br><span class="line">    <span class="comment"># 迁移脚本的文件名分隔符，默认__</span></span><br><span class="line">    <span class="comment">#sql-migration-separator:</span></span><br><span class="line">    <span class="comment"># 迁移脚本的后缀，默认为.sql</span></span><br><span class="line">    <span class="comment">#sql-migration-suffix:</span></span><br><span class="line">    <span class="comment"># 使用的元数据表名，默认为schema_version</span></span><br><span class="line">    <span class="comment">#table: flyway_schema_history</span></span><br><span class="line">    <span class="comment"># 迁移时使用的目标版本，默认为latest version</span></span><br><span class="line">    <span class="comment">#target:</span></span><br></pre></td></tr></table></figure>

<p>flyway下没有配置url、user、password的话将会使用springboot的数据源。</p>
<h3 id="3、在db-migration包下新建V1-initialization-table-sql文件。"><a href="#3、在db-migration包下新建V1-initialization-table-sql文件。" class="headerlink" title="3、在db.migration包下新建V1__initialization_table.sql文件。"></a>3、在db.migration包下新建V1__initialization_table.sql文件。</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="keyword">user</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`user`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">unsigned</span> zerofill <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'自增ID'</span>,</span><br><span class="line">  <span class="string">`user_name`</span> <span class="built_in">varchar</span>(<span class="number">64</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8mb4 <span class="keyword">COLLATE</span> utf8mb4_general_ci <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'姓名'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8mb4 <span class="keyword">COLLATE</span>=utf8mb4_general_ci ROW_FORMAT=DYNAMIC <span class="keyword">COMMENT</span>=<span class="string">'用户表'</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>flyway脚本默认放在classpath:/db/migration目录下面，如果想换位置，可以自定义一个位置spring.flyway.locations即为脚本存放的位置。</p>
</blockquote>
<h3 id="4、开始验证"><a href="#4、开始验证" class="headerlink" title="4、开始验证"></a>4、开始验证</h3><p>目前只有test库中没有任何表：</p>
<p><img src="http://img.rogermaster.top/uPic/JEo9XF.png" alt="JEo9XF"></p>
<p>我们开始启动项目看看：</p>
<p><img src="http://img.rogermaster.top/uPic/NQVITI.png" alt="NQVITI"></p>
<p>通过日志信息，我们可以看到已经迁移成功了，我们去数据库看看。</p>
<p><img src="http://img.rogermaster.top/uPic/PI8774.png" alt="PI8774"></p>
<p>表结构：</p>
<p><img src="http://img.rogermaster.top/uPic/UTMMNl.png" alt="UTMMNl"></p>
<p>现在我们来给user表新增一个字段试试。</p>
<p>V1.1__addField_col.sql</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="keyword">user</span> <span class="keyword">add</span> mail <span class="built_in">varchar</span>(<span class="number">128</span>) <span class="keyword">comment</span> <span class="string">'用户邮箱'</span>;</span><br></pre></td></tr></table></figure>

<p>运行项目看看</p>
<p><img src="http://img.rogermaster.top/uPic/hIlzSt.png" alt="hIlzSt"></p>
<p>表结构：</p>
<p><img src="http://img.rogermaster.top/uPic/IDeE9S.png" alt="IDeE9S"></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/SpringBoot优雅的参数校验-validation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/SpringBoot优雅的参数校验-validation/" class="post-title-link" itemprop="url">SpringBoot优雅的参数校验-validation</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-07-25 14:28:55" itemprop="dateCreated datePublished" datetime="2020-07-25T14:28:55+08:00">2020-07-25</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-07-26 15:09:35" itemprop="dateModified" datetime="2020-07-26T15:09:35+08:00">2020-07-26</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spring-boot/" itemprop="url" rel="index"><span itemprop="name">spring boot</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SpringBoot优雅的参数校验-validation"><a href="#SpringBoot优雅的参数校验-validation" class="headerlink" title="SpringBoot优雅的参数校验-validation"></a>SpringBoot优雅的参数校验-validation</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&ensp;&ensp;&ensp;&ensp;在后端开发过程中，我们避免不了对前端传过来的参数进行检验。有的人会说，前端检验不就行了吗，为啥还要后端检验一遍？我们前端经常对我说的一句话：”后端不要信任前端，你怎么知道接口不会被拦截，前端就一定不会传错。“。因此，参数校验是非常重要的一个环节，严格参数校验会减少很多出bug的概率，增加接口的安全性，增强程序的健壮性。</p>
<p>&ensp;&ensp;&ensp;&ensp;当我们在做参数检验的时候，可能会出现这种情况：参数太多，我们要写很多条件判断语句，这样就会显得代码不够整洁，阅读体验也不是很好。<br><img src="http://img.rogermaster.top/uPic/SKO0yt.png" alt="SKO0yt"><br>&ensp;&ensp;&ensp;&ensp;因此我们需要一种优雅的方式来处理上面的问题，进行SpringBoot统一参数校验。那就是今天我们的主角validation啦。</p>
<h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><p>&ensp;&ensp;&ensp;&ensp;通过@Validated这一注解配合一些参数校验注解(PS:@NotNull，@NotEmpty)。然后对抛出的异常进行全局统一捕获然后返回错误信息。</p>
<h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Validation --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-validation<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="PostParams"><a href="#PostParams" class="headerlink" title="PostParams"></a>PostParams</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.spring_boot_validation.entity.params;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Data;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.validation.Valid;</span><br><span class="line"><span class="keyword">import</span> javax.validation.constraints.NotEmpty;</span><br><span class="line"><span class="keyword">import</span> javax.validation.constraints.NotNull;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Roger</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: post参数类</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>: 2020/7/25 4:31 下午</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PostParams</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * ID</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@NotNull</span>(message = <span class="string">"ID不能为空"</span>)</span><br><span class="line">    <span class="keyword">private</span> Integer id;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 名称</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@NotNull</span>(message = <span class="string">"名称不能为空"</span>)</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@NotEmpty</span>(message = <span class="string">"数组里面至少有一个元素"</span>)</span><br><span class="line">    <span class="keyword">private</span> List&lt;Integer&gt; array;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Valid</span></span><br><span class="line">    <span class="meta">@NotNull</span>(message = <span class="string">"item不能为空"</span>)</span><br><span class="line">    <span class="keyword">private</span> Item item;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="TestController"><a href="#TestController" class="headerlink" title="TestController"></a>TestController</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@Validated</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/get-test"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResponseEntity&lt;Object&gt; <span class="title">getTest</span><span class="params">(@RequestParam(required = <span class="keyword">false</span>)</span> @<span class="title">NotNull</span><span class="params">(message = <span class="string">"offset不能为空"</span>)</span> Integer offset,</span></span><br><span class="line"><span class="function">                                          @<span class="title">RequestParam</span><span class="params">(required = <span class="keyword">false</span>)</span> @<span class="title">NotNull</span><span class="params">(message = <span class="string">"limit不能为空"</span>)</span> Integer limit)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResponseEntity&lt;&gt;(<span class="string">"ok"</span>, HttpStatus.OK);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostMapping</span>(<span class="string">"/post-test"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResponseEntity&lt;Object&gt; <span class="title">postTest</span><span class="params">(@Valid @RequestBody PostParams postParams)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResponseEntity&lt;&gt;(HttpStatus.CREATED);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/get-test2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResponseEntity&lt;Object&gt; <span class="title">getTest2</span><span class="params">(@RequestParam(required = <span class="keyword">false</span>)</span> @<span class="title">NotNull</span><span class="params">(message = <span class="string">"offset不能为空"</span>)</span> Integer offset,</span></span><br><span class="line"><span class="function">                                          @<span class="title">RequestParam</span><span class="params">(required = <span class="keyword">false</span>)</span> Integer limit)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResponseEntity&lt;&gt;(<span class="string">"ok"</span>, HttpStatus.OK);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/get-test3"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResponseEntity&lt;Object&gt; <span class="title">getTest3</span><span class="params">(@Valid Item item)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResponseEntity&lt;&gt;(<span class="string">"ok"</span>, HttpStatus.OK);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/get-test4"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResponseEntity&lt;Object&gt; <span class="title">getTest4</span><span class="params">(@NotNull(message = <span class="string">"offset不能为空"</span>)</span> Integer offset,</span></span><br><span class="line"><span class="function">                                           Integer limit)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResponseEntity&lt;&gt;(<span class="string">"ok"</span>, HttpStatus.OK);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ControllerAdvice"><a href="#ControllerAdvice" class="headerlink" title="ControllerAdvice"></a>ControllerAdvice</h3><p>参数检验异常统一拦截处理</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestControllerAdvice</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ControllerAdvice</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//参数检验错误 @RequestParam上validate失败后抛出的异常是javax.validation.ConstraintViolationException</span></span><br><span class="line">    <span class="meta">@ResponseStatus</span>(HttpStatus.BAD_REQUEST)</span><br><span class="line">    <span class="meta">@ExceptionHandler</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResultVo&lt;Object&gt; <span class="title">handlerMethodArgumentNotValidException</span><span class="params">(<span class="keyword">final</span> MethodArgumentNotValidException e)</span> </span>&#123;</span><br><span class="line">        List&lt;ObjectError&gt; objectErrors = e.getBindingResult().getAllErrors();</span><br><span class="line">        StringBuilder errorMessages = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        objectErrors.forEach(objectError -&gt; errorMessages.append(objectError.getDefaultMessage()).append(<span class="string">";"</span>));</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResultVo&lt;&gt;(<span class="number">0</span>,String.valueOf(errorMessages),<span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//参数检验错误 @RequestBody上validate失败后抛出的异常是MethodArgumentNotValidException异常。</span></span><br><span class="line">    <span class="meta">@ResponseStatus</span>(HttpStatus.BAD_REQUEST)</span><br><span class="line">    <span class="meta">@ExceptionHandler</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResultVo&lt;Object&gt; <span class="title">handlerConstraintViolationException</span> <span class="params">(<span class="keyword">final</span> ConstraintViolationException e)</span> </span>&#123;</span><br><span class="line">        String errorMessages = e.getConstraintViolations().stream().map(ConstraintViolation::getMessage).collect(Collectors.joining(<span class="string">";"</span>));</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResultVo&lt;&gt;(<span class="number">0</span>,String.valueOf(errorMessages),<span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//参数检验错误 validate失败后抛出的异常是BindException异常。</span></span><br><span class="line">    <span class="meta">@ResponseStatus</span>(HttpStatus.BAD_REQUEST)</span><br><span class="line">    <span class="meta">@ExceptionHandler</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResultVo&lt;Object&gt; <span class="title">handlerConstraintViolationException</span> <span class="params">(<span class="keyword">final</span> BindException e)</span> </span>&#123;</span><br><span class="line">        String errorMessages = e.getBindingResult().getAllErrors().stream().map(ObjectError::getDefaultMessage).collect(Collectors.joining(<span class="string">";"</span>));</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ResultVo&lt;&gt;(<span class="number">0</span>,String.valueOf(errorMessages),<span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​    在使用参数检验的过程中，主要会抛出以上三种异常BindException、MethodArgumentNotValidException、ConstraintViolationException。原因主要是因为跟请求发起的数据格式（content-type）有关系，对于不同的传输数据的格式spring采用不同的HttpMessageConverter（http参数转换器）来进行处理。</p>
<p>​    请求体(@RequestBody)绑定到java bean上失败时抛出MethodArgumentNotValidException；普通参数(@RequestParam)(非 java bean)校验出错时抛出ConstraintViolationException；请求参数绑定到java bean上失败时抛出BindException；</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>请求POST: <a href="http://localhost:8080/post-test" target="_blank" rel="noopener">http://localhost:8080/post-test</a> ,结果如下</p>
<p><img src="http://img.rogermaster.top/uPic/QwBokF.png" alt="QwBokF"></p>
<h3 id="常用的一些注解解析"><a href="#常用的一些注解解析" class="headerlink" title="常用的一些注解解析"></a>常用的一些注解解析</h3><h5 id="Validated-和-Valid-的异同"><a href="#Validated-和-Valid-的异同" class="headerlink" title="@Validated 和 @Valid 的异同"></a>@Validated 和 @Valid 的异同</h5><table>
<thead>
<tr>
<th align="left"><strong>注解</strong></th>
<th align="left"><strong>范围</strong></th>
<th><strong>嵌套</strong></th>
<th><strong>校验组</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">@Validated</td>
<td align="left">可以标记类、方法、方法参数，不能用在成员属性（字段）上</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td align="left">@Valid</td>
<td align="left">可以标记方法、构造函数、方法参数和成员属性（字段）上</td>
<td>支持</td>
<td>不支持</td>
</tr>
</tbody></table>
<p>通常在使用过程中，我们把@Validated标记在类上，然后@Valid标记在实体中的属性上。在Controller中使用，把@Validated标记在类上。然后针对java bean的参数就用@Valid注解。如下所示：</p>
<p><img src="http://img.rogermaster.top/uPic/0LcqcD.png" alt="0LcqcD"></p>
<h4 id="校验注解一览表"><a href="#校验注解一览表" class="headerlink" title="校验注解一览表"></a>校验注解一览表</h4><table>
<thead>
<tr>
<th><strong>注解</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody><tr>
<td>@Valid</td>
<td>被注释的元素是一个对象，需要检查此对象的所有字段值</td>
</tr>
<tr>
<td>@Null</td>
<td>被注释的元素必须为 null</td>
</tr>
<tr>
<td>@NotNull</td>
<td>被注释的元素必须不为 null</td>
</tr>
<tr>
<td>@AssertTrue</td>
<td>被注释的元素必须为 true</td>
</tr>
<tr>
<td>@AssertFalse</td>
<td>被注释的元素必须为 false</td>
</tr>
<tr>
<td>@Min(value)</td>
<td>被注释的元素必须是一个数字，其值必须大于等于指定的最小值</td>
</tr>
<tr>
<td>@Max(value)</td>
<td>被注释的元素必须是一个数字，其值必须小于等于指定的最大值</td>
</tr>
<tr>
<td>@DecimalMin(value)</td>
<td>被注释的元素必须是一个数字，其值必须大于等于指定的最小值</td>
</tr>
<tr>
<td>@DecimalMax(value)</td>
<td>被注释的元素必须是一个数字，其值必须小于等于指定的最大值</td>
</tr>
<tr>
<td>@Size(max, min)</td>
<td>被注释的元素的大小必须在指定的范围内</td>
</tr>
<tr>
<td>@Digits (integer, fraction)</td>
<td>被注释的元素必须是一个数字，其值必须在可接受的范围内</td>
</tr>
<tr>
<td>@Past</td>
<td>被注释的元素必须是一个过去的日期</td>
</tr>
<tr>
<td>@Future</td>
<td>被注释的元素必须是一个将来的日期</td>
</tr>
<tr>
<td>@Pattern(value)</td>
<td>被注释的元素必须符合指定的正则表达式</td>
</tr>
<tr>
<td>@Email</td>
<td>被注释的元素必须是电子邮箱地址</td>
</tr>
<tr>
<td>@Length(min=, max=)</td>
<td>被注释的字符串的大小必须在指定的范围内</td>
</tr>
<tr>
<td>@NotEmpty</td>
<td>被注释的字符串的必须非空</td>
</tr>
<tr>
<td>@Range(min=, max=)</td>
<td>被注释的元素必须在合适的范围内</td>
</tr>
<tr>
<td>@NotBlank</td>
<td>被注释的字符串的必须非空</td>
</tr>
<tr>
<td>@URL(protocol=,host=, port=,regexp=, flags=)</td>
<td>被注释的字符串必须是一个有效的url</td>
</tr>
</tbody></table>
<h3 id="嵌套验证"><a href="#嵌套验证" class="headerlink" title="嵌套验证"></a>嵌套验证</h3><p>我们很多时候会存在这样的业务场景，前端会给后端传递一个list，我们不仅要限制每次请求list内的个数，同时还要对list内基本元素的属性值进行校验。这个时候就需要进行嵌套验证了，实现的方式很简单。在list上添加@Vaild就可以实现了。</p>
<p><img src="http://img.rogermaster.top/uPic/UqPx8n.png" alt="UqPx8n"></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/2019京东Java校招笔试/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/2019京东Java校招笔试/" class="post-title-link" itemprop="url">2019京东Java校招笔试</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-24 12:15:35 / 修改时间：13:50:46" itemprop="dateCreated datePublished" datetime="2019-08-24T12:15:35+08:00">2019-08-24</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Java/面试准备/" itemprop="url" rel="index"><span itemprop="name">面试准备</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>1 TCP协议的拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。常用的方法有:<br>正确答案 : BC 您的答案 : BC<br>A 慢启动、窗口滑动<br>B 慢开始、拥塞控制<br>C 快重传、快恢复<br>D 快开始、快恢复</p>
<p>2 Shell 脚本（shell script），是一种为 shell 编写的脚本程序。现有一个test.sh文件，且有可执行权限，文件中内容为：<br>#!/bin/bash<br>aa=’Hello World !’<br>请问下面选项中哪个能正常显示Hello World !<br>正确答案 : D 您的答案 : D<br>A sh test.sh &gt;/dev/null 1 &amp;&amp; echo $aa<br>B ./test.sh &gt;/dev/null 1 &amp;&amp; echo $aa<br>C bash test.sh &gt;/dev/null 1 &amp;&amp; echo $aa<br>D source test.sh &gt;/dev/null 1 &amp;&amp; echo $aa</p>
<p>3 bash脚本文件一般第一行开头是<br>正确答案 : C 您的答案 : C<br>A //<br>B ##<br>C #!<br>D #/</p>
<p>4 在bash编程中,算术比较大于、大于等于的运算符是（ ）<br>正确答案 : CD 您的答案 : CD<br>A &gt;<br>B &gt;=<br>C ge<br>D gt</p>
<p>5 系统管理员编写扫描临时文件的shell程序tmpsc.sh, 测试该程序时提示拒绝执行，解决的方法有（ ）<br>正确答案 : BCD 您的答案 : BC<br>A chmod 644 tmpsc.sh<br>B chmod 755 tmpsc.sh<br>C chmod a+x tmpsc.sh<br>D chmod u+x tmpsc.sh<br>解析：<br>目录/文件的满权限的形式：<br>drwxrwxrwx<br>-rwxrwxrwx 其中：(r:读取，w:写，x:执行)<br>数字对应：(r:4，w:2，x:1)， 命令行中的三个数字对应的授权角色为owner, group, others</p>
<ol>
<li>通过数字修改权限<br>chmod 777 [-R]</li>
<li>符号类型修改<br>u: owner<br>g: group<br>o: others<br>a : all<br>chmod a+x [-R] 所有人都拥有执行权限</li>
</ol>
<p>6 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.ArrayBlockingQueue;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ThreadPoolExecutor;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> ThreadPoolExecutor executor = <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, TimeUnit.SECONDS,</span><br><span class="line"> <span class="keyword">new</span> ArrayBlockingQueue&lt;Runnable&gt;(<span class="number">5</span>), <span class="keyword">new</span> ThreadPoolExecutor.CallerRunsPolicy());</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>线程池executor在空闲状态下的线程个数是？<br>正确答案 : B 您的答案 : B<br>A 0<br>B 5<br>C 10<br>D 不确定</p>
<p>7 Object类不含有以下哪种方法？<br>正确答案 : A 您的答案 : A<br>A equal<br>B wait<br>C notify<br>D clone</p>
<p>8</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"> <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">100</span>;i++)&#123;</span><br><span class="line"> list.add(<span class="string">"a"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>JDK1.8中，执行以上程序后，该list进行了几次扩容？<br>正确答案 : C 您的答案 : D<br>A 4<br>B 5<br>C 6<br>D 7<br>解析：初始容量被设置为10,那么容量变化的规则是((旧容量 * 3) / 2) + 1，也就是1.5倍增长</p>
<p>9 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> System.out.print(fun1());</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">fun1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> <span class="keyword">try</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"A"</span>);</span><br><span class="line"> <span class="keyword">return</span> fun2();</span><br><span class="line"> &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"B"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">fun2</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> System.out.print(<span class="string">"C"</span>);</span><br><span class="line"> <span class="keyword">return</span> <span class="string">"D"</span>;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行以上程序后，输出结果正确的是？<br>正确答案 : C 您的答案 : C<br>A ABCD<br>B ACDB<br>C ACBD<br>D 不确定</p>
<p>10 根据类加载器加载类的初始化原理，推断以下代码的输入结果为？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"> ClassLoader classLoader=ClassLoader.getSystemClassLoader();</span><br><span class="line"> Class clazz=classLoader.loadClass(<span class="string">"A"</span>);</span><br><span class="line"> System.out.print(<span class="string">"Test"</span>);</span><br><span class="line"> clazz.forName(<span class="string">"A"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span></span>&#123;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"A"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>正确答案 : A 您的答案 : A<br>A TestA<br>B ATestA<br>C ATest<br>D Test</p>
<p>11 继承是JAVA语言的一个特性，针对类的继承，虚拟机会如何进行父类和子类的初始化加载呢？请阅读代码选择出该段代码<br>的输入结果。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> System.out.print(B.c);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> </span>&#123;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> String c = <span class="string">"C"</span>;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"A"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> <span class="keyword">extends</span> <span class="title">A</span></span>&#123;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"B"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>正确答案 : A 您的答案 : A<br>A AC<br>B ABC<br>C C<br>D BC</p>
<p>12 继承是JAVA语言的一个特性，针对类的继承，虚拟机会如何进行父类和子类的初始化加载呢？请阅读代码选择出该段代码<br>的输入结果。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> System.out.print(B.c);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> </span>&#123;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"A"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> <span class="keyword">extends</span> <span class="title">A</span></span>&#123;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"B"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> String c = <span class="string">"C"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>正确答案 : C 您的答案 : C<br>A AB<br>B ABC<br>C C<br>D BC</p>
<p>13 JAVA的类加载期负责整个生命周期内的class的初始化和加载工作，就虚拟机的规范来说，以下代码会输出什么结果？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> System.out.println(Test2.a);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test2</span></span>&#123;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"OK"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String a=<span class="keyword">new</span> String(<span class="string">"JD"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>正确答案 : D 您的答案 : D<br>A 只有JD<br>B 只有OK<br>C 输出 JDOK<br>D 输出 OKJD</p>
<p>14 JAVA的类加载期负责整个生命周期内的class的初始化和加载工作，就虚拟机的规范来说，以下代码会输出什么结果？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> System.out.println(Test2.a);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test2</span></span>&#123;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String a=<span class="keyword">new</span> String(<span class="string">"JD"</span>);</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"OK"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>正确答案 : D 您的答案 : D<br>A 只有JD<br>B 只有OK<br>C 输出 JDOK<br>D 输出 OKJD</p>
<p>15 JAVA的类加载期负责整个生命周期内的class的初始化和加载工作，就虚拟机的规范来说，以下代码会输出什么结果？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> System.out.println(Test2.a);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test2</span></span>&#123;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String a=<span class="string">"JD"</span>;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.print(<span class="string">"OK"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>正确答案 : A 您的答案 : A<br>A 只有JD<br>B 只有OK<br>C 输出 JDOK<br>D 输出 OKJD</p>
<p>16 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> String s1 = <span class="string">"abc"</span>;</span><br><span class="line"> String s2 = <span class="string">"abc"</span>;</span><br><span class="line"> System.out.println(s1 == s2);</span><br><span class="line"> String s3 = <span class="keyword">new</span> String(<span class="string">"abc"</span>);</span><br><span class="line"> System.out.println(s1 == s3);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行以上程序后，输出结果正确的是？<br>正确答案 : B 您的答案 : B<br>A true true<br>B true false<br>C false fasle<br>D false true</p>
<p>17 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> x = <span class="number">10</span>;</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">static</span> Integer y = <span class="number">10</span>;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">updateX</span><span class="params">(<span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line"> value = <span class="number">3</span> * value;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">updateY</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line"> value = <span class="number">3</span> * value;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> updateX(x);</span><br><span class="line"> updateY(y);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行以上程序后，x和y的值分别是多少？<br>正确答案 : A 您的答案 : A<br>A 10,10<br>B 10,30<br>C 30,10<br>D 30,30</p>
<p>18 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> System.out.println(<span class="string">"A"</span>);</span><br><span class="line"> <span class="keyword">new</span> Main();</span><br><span class="line"> <span class="keyword">new</span> Main();</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">Main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> System.out.println(<span class="string">"B"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> &#123;</span><br><span class="line"> System.out.println(<span class="string">"C"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> System.out.println(<span class="string">"D"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>以上程序输出的结果，正确的是？<br>正确答案 : C 您的答案 : C<br>A DCABB<br>B DABCBC<br>C DACBCB<br>D DACBB</p>
<p>19 兼容接口不同的类在一起工作，采用以下哪种设计模式最好？<br>正确答案 : B 您的答案 : C<br>A 建造者模式<br>B 适配器模式<br>C 桥接模式<br>D 代理模式</p>
<p>20 下图的UML类结构图表示的是哪种设计模式：<br><img src="http://image.acmcoder.com/v1/acmimg/8750747399346469.png?authorization=bce-auth-v1%2F02fe1db0eea94e8480054b43acd6124f%2F2019-02-20T08%3A25%3A05Z%2F-1%2F%2F35eff08031c0d50f671b070ae7e081ac9cce55b98bbf3c39e5f2f5e1590211ef" alt="图"><br>正确答案 : A 您的答案 : A<br>A 抽象工厂模式<br>B 享元模式<br>C 装饰模式<br>D 责任链模式</p>
<p>21 以下哪条SQL语句可以返回table1中的全部的key：<br>正确答案 : D 您的答案 : D<br>A select tabel1.key from table1 join tabel2 on table1.key=table2.key<br>B select tabel1.key from table1 right outer join tabel2 on table1.key=table2.key<br>C select tabel1.key from table1 left semi join tabel2 on table1.key=table2.key<br>D select tabel1.key from table1 left outer join tabel2 on table1.key=table2.key</p>
<p>22 下列有关软连接描述正确的是<br>正确答案 : C 您的答案 : C<br>A 与普通文件没什么不同，inode 都指向同一个文件在硬盘中的区块<br>B 不能对目录创建软链接<br>C 保存了其代表的文件的绝对路径，是另外一种文件，在硬盘上有独立的区块，访问时替换自身路径<br>D 不可以对不存在的文件创建软链接</p>
<p>23 以下哪种设备工作在数据链路层？<br>正确答案 : B 您的答案 : B<br>A 中继器<br>B 集线器<br>C 交换机<br>D 路由器</p>
<p>24 一颗二叉树的叶子节点有5个，出度为1的结点有3个，该二叉树的结点总个数是？<br>正确答案 : B 您的答案 : B<br>A 11<br>B 12<br>C 13<br>D 14</p>
<p>25 若串S=”UP！UP！JD”，则其子串的数目<br>正确答案 : B 您的答案 : B<br>A 33<br>B 37<br>C 39<br>D 35<br>解析：(n*(n+1)/2)+1</p>
<p>26 已知小顶堆：{51,32,73,23,42,62,99,14,24,3943,58,65,80,120}，请问62对应节点的左子节点是<br>正确答案 : B 您的答案 : B<br>A 99<br>B 73<br>C 3943<br>D 120</p>
<p>27 关于递归法的说法不正确的是（ ）<br>正确答案 : D 您的答案 : D<br>A 程序结构更简洁<br>B 占用CPU的处理时间更多<br>C 要消耗大量的内存空间，程序执行慢，甚至无法执行<br>D 递归法比递推法的执行效率更高</p>
<p>28 以下为求0到1000以内所有奇数和的算法，从中选出描述正确的算法（ ）<br>正确答案 : A 您的答案 : A<br>A ①s=0；②i=1；③s=s+i；④i=i+2；⑤如果i≤1000，则返回③；⑥结束<br>B ①s=0；②i=1；③i=i+2；④s=s+i；⑤如果i≤1000，则返回③；⑥结束<br>C ①s=1；②i=1；③s=s+i；④i=i+2；⑤如果i≤1000，则返回③；⑥结束<br>D ①s=1；②i=1；③i=i+2；④s=s+i；⑤如果i≤1000，则返回③；⑥结束</p>
<p>29 如何在多线程中避免发生死锁？<br>正确答案 : ABCD 您的答案 : ABCD<br>A 允许进程同时访问某些资源。<br>B 允许进程强行从占有者那里夺取某些资源。<br>C 进程在运行前一次性地向系统申请它所需要的全部资源。<br>D 把资源事先分类编号，按号分配，使进程在申请，占用资源时不会形成环路。</p>
<p>30 下面有关值类型和引用类型描述正确的是（）？<br>正确答案 : ABC 您的答案 : ABC<br>值类型的变量赋值只是进行数据复制，创建一个同值的新对象，而引用类型变量赋值，仅仅是把对象的引用<br>的指针赋值给变量，使它们共用一个内存地址。<br>A<br>值类型数据是在栈上分配内存空间，它的变量直接包含变量的实例，使用效率相对较高。而引用类型数据是<br>分配在堆上，引用类型的变量通常包含一个指向实例的指针，变量通过指针来引用实例。<br>B<br>C 引用类型一般都具有继承性，但是值类型一般都是封装的，因此值类型不能作为其他任何类型的基类。<br>D 值类型变量的作用域主要是在栈上分配内存空间内，而引用类型变量作用域主要在分配的堆上。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/passages/tensorflow2-0入门系列（二）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Roger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Roger's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/passages/tensorflow2-0入门系列（二）/" class="post-title-link" itemprop="url">tensorflow2.0入门系列（二）</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-06 10:21:35" itemprop="dateCreated datePublished" datetime="2019-08-06T10:21:35+08:00">2019-08-06</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-24 13:57:24" itemprop="dateModified" datetime="2019-08-24T13:57:24+08:00">2019-08-24</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/tensorflow2-0/" itemprop="url" rel="index"><span itemprop="name">tensorflow2.0</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/tensorflow2-0/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="tensorflow2-0入门系列（二）"><a href="#tensorflow2-0入门系列（二）" class="headerlink" title="tensorflow2.0入门系列（二）"></a>tensorflow2.0入门系列（二）</h1><h3 id="导入相关包"><a href="#导入相关包" class="headerlink" title="导入相关包"></a>导入相关包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<p>Fashion MNIST数据集，包含10个类别中的70,000个灰度图像。 图像显示了（28 x 28）的单个服装物品.<br>60,000张图像来训练网络和10,000张图像来评估网络学习图像分类的准确程度<br>图像为28x28 NumPy数组，像素值范围为0到255.标签是一个整数数组，范围从0到9.这些对应于图像所代表的服装类别：</p>
<table>
<thead>
<tr>
<th align="left">Label</th>
<th align="center">Class</th>
</tr>
</thead>
<tbody><tr>
<td align="left">0</td>
<td align="center">T-shirt/top</td>
</tr>
<tr>
<td align="left">1</td>
<td align="center">Trouser</td>
</tr>
<tr>
<td align="left">2</td>
<td align="center">Pullover</td>
</tr>
<tr>
<td align="left">3</td>
<td align="center">Dress</td>
</tr>
<tr>
<td align="left">4</td>
<td align="center">Coat</td>
</tr>
<tr>
<td align="left">5</td>
<td align="center">Sandal</td>
</tr>
<tr>
<td align="left">6</td>
<td align="center">Shirt</td>
</tr>
<tr>
<td align="left">7</td>
<td align="center">Sneaker</td>
</tr>
<tr>
<td align="left">8</td>
<td align="center">Bag</td>
</tr>
<tr>
<td align="left">9</td>
<td align="center">Ankle boot</td>
</tr>
</tbody></table>
<h3 id="导入时装MNIST数据集，作为MNIST手写时装数据集的扩展"><a href="#导入时装MNIST数据集，作为MNIST手写时装数据集的扩展" class="headerlink" title="导入时装MNIST数据集，作为MNIST手写时装数据集的扩展"></a>导入时装MNIST数据集，作为MNIST手写时装数据集的扩展</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line"></span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</span><br></pre></td></tr></table></figure>

<p>每个图像都映射到一个标签。 由于类名不包含在数据集中，因此将它们存储在此处以便在后面绘制图像时使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#新建列表用于存儲每個标签所对应的类型</span></span><br><span class="line">class_names = [<span class="string">'T-shirt/top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,<span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle boot'</span>]</span><br></pre></td></tr></table></figure>

<h3 id="数据扩展"><a href="#数据扩展" class="headerlink" title="数据扩展"></a>数据扩展</h3><p>（ps下方展示的代码仅可用于了解一些基本的数据操作扩展方法，并非核心代码）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取数据集的Shape</span></span><br><span class="line">print(train_images)</span><br><span class="line"><span class="comment">#输出(60000, 28, 28)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#获取数据集的数量</span></span><br><span class="line">print(len(train_images))</span><br><span class="line"><span class="comment">#输出60000</span></span><br></pre></td></tr></table></figure>

<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>在训练网络之前必须对数据进行预处理。<br>一是方便我们的神经网络能够快速的识别里面的特征。<br>二是能够有效的帮助我们训练出一个成功的模型。<br>我们可以借助matplotlib这个绘图工具包帮助我们直观的感受数据集，以及训练过程。当然TensorFlow也提供了tensorboard用于可视化，暂时我们先不做讲解<br>我们可以先试着借助matplotlib画出我们数据集中的某张图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line"><span class="comment">#显示图像</span></span><br><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#颜色比例彩条</span></span><br><span class="line">plt.colorbar()</span><br><span class="line"><span class="comment">#不显示网格线</span></span><br><span class="line">plt.grid(<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#保存图像</span></span><br><span class="line">plt.savefig(<span class="string">'some_one.png'</span>)</span><br><span class="line"><span class="comment">#显示图像</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://pvqnwtxgq.bkt.clouddn.com/some_one.png" alt="输出结果"><br>在将它们送到神经网络模型之前，我们为了将这些值缩放到0到1的范围，我们将值除以255.训练集和测试集以相同的方式进行预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_images = train_images / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">test_images = test_images / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>

<p>为了验证数据格式是否正确以及我们是否已准备好构建和训练网络，让我们显示训练集中的前25个图像并在每个图像下方显示类名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">25</span>):</span><br><span class="line">    <span class="comment">#创建子图，5行5列，第i+1个</span></span><br><span class="line">    plt.subplot(<span class="number">5</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#X轴坐标刻度设置</span></span><br><span class="line">    plt.xticks([])</span><br><span class="line">    <span class="comment">#Y轴坐标刻度设置</span></span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.grid(<span class="literal">False</span>)</span><br><span class="line">    <span class="comment">#cmap: 颜色图谱（colormap), 默认绘制为RGB(A)颜色空间，这里使用的是二值图</span></span><br><span class="line">    plt.imshow(train_images[i])</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    <span class="comment">#设置X轴标签</span></span><br><span class="line">    plt.xlabel(class_names[train_labels[i]])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>可以通过右边的彩色比例条可以明显看出图像值已经缩放到0-1的范围</p>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="设置神经网络层"><a href="#设置神经网络层" class="headerlink" title="设置神经网络层"></a>设置神经网络层</h4><p>这里还是采用Keras提供的层叠模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([</span><br><span class="line">    <span class="comment"># 输入层</span></span><br><span class="line">    keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">    <span class="comment"># 隐藏层一，全连接</span></span><br><span class="line">    keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    <span class="comment"># 输出层，全连接</span></span><br><span class="line">    keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h4 id="配置模型"><a href="#配置模型" class="headerlink" title="配置模型"></a>配置模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采用Adam优化器，loss计算方法为sparse_categorical_crossentropy，评价函数</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>

<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练总轮数为5轮</span></span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>看准确度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nTest accuracy:'</span>, test_acc)</span><br></pre></td></tr></table></figure>

<h3 id="进行预测"><a href="#进行预测" class="headerlink" title="进行预测"></a>进行预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions = model.predict(test_images)</span><br></pre></td></tr></table></figure>

<p>现在我们去其中第一个结果看看预测情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>输出：[2.4625590e-08 6.4621108e-10 6.6212579e-08 1.8379983e-11 4.6871547e-09<br>4.0910607e-05 3.9370971e-07 9.7850722e-04 2.8181713e-09 9.9898010e-01]<br>预测是10个数字的数组。 它们代表模型的“信心”，即图像对应于10种不同服装中的每一种。 我们可以看到哪个标签具有最高的置信度值<br>我们可以使用np.argmax()来直接计算出置信度最高的所对应的标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.argmax(predictions[<span class="number">0</span>]))</span><br><span class="line"><span class="comment">#输出 9</span></span><br></pre></td></tr></table></figure>

<p>我们再对比一下真正的标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(test_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 输出 9</span></span><br></pre></td></tr></table></figure>

<p>我们同样可以借助matplotlib直观的表现出来<br>首先我们可以构建画小图的函数和画预测值直方图的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span><span class="params">(i, predictions_array, true_label, img)</span>:</span></span><br><span class="line">  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]</span><br><span class="line">  plt.grid(<span class="literal">False</span>)</span><br><span class="line">  plt.xticks([])</span><br><span class="line">  plt.yticks([])</span><br><span class="line"></span><br><span class="line">  plt.imshow(img, cmap=plt.cm.binary)</span><br><span class="line"></span><br><span class="line">  predicted_label = np.argmax(predictions_array)</span><br><span class="line">  <span class="keyword">if</span> predicted_label == true_label:</span><br><span class="line">    color = <span class="string">'blue'</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    color = <span class="string">'red'</span></span><br><span class="line"></span><br><span class="line">  plt.xlabel(<span class="string">"&#123;&#125; &#123;:2.0f&#125;% (&#123;&#125;)"</span>.format(class_names[predicted_label],</span><br><span class="line">                                <span class="number">100</span>*np.max(predictions_array),</span><br><span class="line">                                class_names[true_label]),</span><br><span class="line">                                color=color)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_value_array</span><span class="params">(i, predictions_array, true_label)</span>:</span></span><br><span class="line">  predictions_array, true_label = predictions_array[i], true_label[i]</span><br><span class="line">  plt.grid(<span class="literal">False</span>)</span><br><span class="line">  plt.xticks(range(<span class="number">10</span>), class_names, rotation=<span class="number">90</span>)</span><br><span class="line">  plt.yticks([])</span><br><span class="line">  thisplot = plt.bar(range(<span class="number">10</span>), predictions_array, color=<span class="string">"#777777"</span>)</span><br><span class="line">  plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">  predicted_label = np.argmax(predictions_array)</span><br><span class="line"></span><br><span class="line">  thisplot[predicted_label].set_color(<span class="string">'red'</span>)</span><br><span class="line">  thisplot[true_label].set_color(<span class="string">'blue'</span>)</span><br></pre></td></tr></table></figure>

<p>我们可以看看15张图片的表现情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_rows = <span class="number">5</span></span><br><span class="line">num_cols = <span class="number">3</span></span><br><span class="line">num_images = num_rows*num_cols</span><br><span class="line">plt.figure(figsize=(<span class="number">2</span>*<span class="number">2</span>*num_cols, <span class="number">2</span>*num_rows))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_images):</span><br><span class="line">  plt.subplot(num_rows, <span class="number">2</span>*num_cols, <span class="number">2</span>*i+<span class="number">1</span>)</span><br><span class="line">  plot_image(i, predictions, test_labels, test_images)</span><br><span class="line">  plt.subplot(num_rows, <span class="number">2</span>*num_cols, <span class="number">2</span>*i+<span class="number">2</span>)</span><br><span class="line">  plot_value_array(i, predictions, test_labels)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://pvqnwtxgq.bkt.clouddn.com/15_pre.png" alt="预测情况"></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
    <img class="site-author-image" itemprop="image" src="/uploads/avatar.jpeg" alt="Roger">
  
  <p class="site-author-name" itemprop="name">Roger</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    

    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>













          
          
        </div>
      </div>

      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">  <a href="http://www.beian.miit.gov.cn" rel="noopener" target="_blank">渝ICP备19010391号-1 </a>&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Roger</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>



  

  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  



  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
